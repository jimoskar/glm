---
title: 'TMA4315: Project 1'
author: "Jim Totland, Martin Gudahl Tufte"
date: "9/8/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1
## a)
Since the response variables $y_i \sim \mathrm{Bernoulli}(\pi_i)$, where $\pi_i = \Pr(y_i=1 \mid \boldsymbol{x}_i)$. The conditional mean is given by $\mathrm{E}y_i = \pi_i$, which is connected to the covariates via the following relationship:

$$
\boldsymbol{x}_i^T\boldsymbol{\beta} =: \eta_i = \Phi^{-1}(\pi_i),
$$
or equivalently: $\pi_i = \Phi(\eta_i)$. This results in the likelihood function

$$
\begin{split}
L(\boldsymbol{\beta}) &= \prod_{i= 1}^n \pi_i^{y_i}(1-\pi_i)^{1-y_i} \\
&=\prod_{i = 1}^n\Phi(\eta_i)^{y_i}(1 - \Phi(\eta_i))^{1-y_i}.
\end{split}
$$
Thus, the log-likelihood becomes

$$
l(\boldsymbol{\beta}) := \mathrm{ln}(L(\boldsymbol{\beta})) = \sum_{i = 1}^n \underbrace{y_i \mathrm{ln}(\Phi(\eta_i)) + (1- y_i)\mathrm{ln}(1-\Phi(\eta_i))}_{= l_i(\boldsymbol{\beta})} = \sum_{i = 1}^nl_i(\boldsymbol{\beta}).
$$
To find the score function, we calculate

$$
\begin{split}
  \frac{\partial l_i(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} &= \frac{y_i}{\Phi(\eta_i)} \frac{\partial
  \Phi(\eta_i)}{\partial  \boldsymbol{\beta}} - \frac{1-y_i}{1-\Phi(\eta_i)} \frac{\partial \Phi(\eta_i)}{\partial \boldsymbol{\beta}} \\
  &= \frac{y_i}{\Phi(\eta_i)} \phi(\eta_i)\boldsymbol{x}_i - \frac{1-y_i}{1-\Phi(\eta_i)} \phi(\eta_i)\boldsymbol{x}_i \\
  &= \frac{y_i(1-\Phi(\eta_i)) - (1-y_i)\Phi(\eta_i)}{\Phi(\eta_i)(1-\Phi(\eta_i))}\phi(\eta_i)\boldsymbol{x}_i \\
  &= \frac{y_i - \Phi(\eta_i)}{\Phi(\eta_i)(1-\Phi(\eta_i))}\phi(\eta_i)\boldsymbol{x}_i.
\end{split}
$$
Consequently, the score function is given by 

$$
\boldsymbol{s}(\boldsymbol{\beta} ) = \sum_{i = 1}^n \frac{y_i - \Phi(\eta_i)}{\Phi(\eta_i)(1-\Phi(\eta_i))}\phi(\eta_i)\boldsymbol{x}_i = X^TD\Sigma^{-1}(\boldsymbol{y} - \boldsymbol{\mu}),
$$

where $D = \text{diag}(\phi(\eta_i))$ and $\Sigma = \text{diag}(\text{Var}(y_i)) = \text{diag}(\Phi(\eta_i)(1-\Phi(\eta_i)))$. Next, we find the expected Fisher information, $F(\boldsymbol{\beta})$. We find it by using the result

$$
\begin{split}
  F(\boldsymbol{\beta}) &= \mathrm{Var}(\boldsymbol{s}(\beta)) = \mathrm{Var} \left( \sum_{i = 1}^n \frac{y_i - \Phi(\eta_i)}{\Phi(\eta_i)(1-\Phi(\eta_i))}\phi(\eta_i)\boldsymbol{x}_i \right) \\
  &= \sum_{i = 1}^n \left[ \frac{\phi(\eta_i)}{\Phi(\eta_i)(1 - \Phi(\eta_i))}\right]^2 \mathrm{Var}(y_i \boldsymbol{x}_i) = \sum_{i = 1}^n\left[ \frac{\phi(\eta_i)}{\Phi(\eta_i)(1 - \Phi(\eta_i))}\right]^2  \boldsymbol{x}_i \mathrm{Var}(y_i) \boldsymbol{x}_i^T \\
  &= \sum_{i = 1}^n \left[ \frac{\phi(\eta_i)}{\Phi(\eta_i)(1 - \Phi(\eta_i))}\right]^2  \pi_i(1-\pi_i) \boldsymbol{x}_i \boldsymbol{x}_i^T = \sum_{i = 1}^n \frac{\phi(\eta_i)^2}{\Phi(\eta_i)(1 - \Phi(\eta_i))} \boldsymbol{x}_i \boldsymbol{x}_i^T,
\end{split}
$$
Where in the third equality we have used that the $y_i$'s are independent. The expected Fisher information can also be verified to have this expression by the general relationship
$$
  F(\boldsymbol{\beta}) = \sum_{i=1}^n \frac{h'(\eta_i)^2}{\mathrm{Var}(y_i)} \boldsymbol{x}_i \boldsymbol{x}_i^T,
$$
where $h'(\eta_i) = \Phi'(\eta_i) = \phi(\eta_i)$ and $\mathrm{Var}(y_i) = \pi_i(1-\pi_i) = \Phi(\eta_i)(1-\Phi(\eta_i))$.


## b) 

The expected Fisher information is given by

$$
  F(\boldsymbol{\beta}) = \sum_{i = 1}^n \frac{\phi(\eta_i)^2}{\Phi(\eta_i)(1 - \Phi(\eta_i))} \boldsymbol{x}_i \boldsymbol{x}_i^T = X^T W X,
$$
where $W = \mathrm{diag}\left(\frac{\phi(\eta_i)^2}{\Phi(\eta_i)(1 - \Phi(\eta_i))}\right)$.

The Fisher scoring algorithm states that the next iterate is given by

$$
  \boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} + F(\boldsymbol{\beta}^{(t)})^{-1} \boldsymbol{s}(\boldsymbol{\beta}^{(t)}).
$$
<!--
Inserting the expected Fisher information and the score function we get 

$$
  \boldsymbol{\beta}^{(t+1)} = (\boldsymbol{x}^T W^{(t)} \boldsymbol{x})^{-1} \boldsymbol{x}^T W^{(t)} \tilde{\boldsymbol{y}}^{(t)},
$$
where the working response vector $\tilde{\boldsymbol{y}}^{(t)}$ has element $i$ given by

$$
  \tilde{y}_i^{(t)} = \boldsymbol{x}_i^T \boldsymbol{\beta}^{(t)} + \frac{y_i - h(\boldsymbol{x}_i^T \boldsymbol{\beta}^{(t)})}{h'(\boldsymbol{x}_i^T \boldsymbol{\beta}^{(t)})} = \eta_i^{(t)} + \frac{y_i - \Phi(\eta_i^{(t)})}{\phi(\eta_i^{(t)})}.
$$
-->

We also need the deviance, which is defined as

 $$
   D = 2( l_{\mathrm{saturated}} - l(\hat{\boldsymbol{\beta}})).
 $$
When we fit a parameter for each data point (which is the case for the saturated model), the result for the Bernoulli distribution is that $\hat{\pi}_i = y_i$. This means that the likelihood function of the saturated model is given by

$$
L_{\mathrm{saturated}} = \prod_{i = 1}^n \hat{\pi}_i^{y_i}(1-\hat{\pi}_i)^{1-y_i} = \prod_{i = 1}^n y_i^{y_i}(1 + y_i)^{1-y_i} = 1,
$$
Where we have used $0^0 = 1$. Consequently, the log-likelihood $l_{\mathrm{saturated}} = \mathrm{ln}(1) =  0$ and the deviance becomes $-2l(\hat{\boldsymbol{\beta}})$. Next follows the Implementation of `myglm` in R:

```{r}
Phi <- function(x) return (pnorm(x))
phi <- function(x) return (dnorm(x))


myglm <- function(formula, data, start = NULL){
  
  # response variable
  resp <- all.vars(formula)[1]
  y <- as.matrix( data[resp] )
  
  
  # model matrix
  X <- model.matrix(formula, data)
  n <- dim(X)[1]
  p <- dim(X)[2]
  
  
  # starting beta
  if (is.null(start)){
    beta = rep(0, p)
  }
  else {
    beta = start
  }
  
  
  # Fisher scoring algorithm
  max_iter <- 50
  tol <- 1e-10
  iter <- 0
  rel.err <- Inf
  
  F.inv = NULL
  eta = NULL
  
  while (rel.err > tol & iter < max_iter){
    # Calculate eta.
    eta <- X %*% beta
    
    # Calculate score.
    D <- diag(as.vector(phi(eta)), n, n)
    Sigma <- diag(as.vector(Phi(eta)*(1 - Phi(eta))), n, n)
    mu.vec <- as.vector(Phi(eta))
    score = t(X) %*% D %*% solve(Sigma) %*% (y - mu.vec)
    
    # Calculate Fisher information and its inverse.
    W <- diag(as.vector(phi(eta)^2 / (Phi(eta)*(1-Phi(eta)))), n, n)
    F <- t(X) %*% W %*% X
    F.inv <- solve(F)
    
    # Update beta.
    beta.new <- beta + F.inv %*% score
    
    iter <- iter + 1
    rel.err <- max(abs(beta.new - beta) / abs(beta.new))
    beta <- beta.new
  }
  
  # Calculating std.errors and deviance.
  std.Error <- sqrt(diag(F.inv))
  deviance = -2 * sum(y*log(pnorm(eta)) + (1 - y)*log(1 -pnorm(eta)))
  
  return (list("coefficients" = data.frame(beta, std.Error),
            "deviance" = deviance,
            "vcov" = F.inv))
}

```

```{r echo = F, eval = T}
# Uses IRWLS
myglm2 <- function(formula, data, start = NULL){
  
  # response variable
  resp <- all.vars(formula)[1]
  y <- as.matrix( data[resp] )
  
  
  # model matrix
  X <- model.matrix(formula, data)
  n <- dim(X)[1]
  p <- dim(X)[2]
  
  
  # starting beta
  if (is.null(start)){
    beta = rep(0, p)
  }
  else {
    beta = start
  }
  
  
  # Fisher scoring algorithm
  max_iter <- 50
  tol <- 1e-10
  iter <- 0
  rel.err <- Inf
  
  while (rel.err > tol & iter < max_iter){
    # calculate eta, y tilde, W
    eta <- X %*% beta
    y.tilde <- eta + (y - Phi(eta)) / (phi(eta))
    W <- diag( as.vector(phi(eta)^2 / (Phi(eta)*(1-Phi(eta)))), n, n )
    
    
    # update beta
    A <- t(X) %*% W %*% X
    b <- t(X) %*% W %*% y.tilde
    beta.new <- solve(A, b)
    
    iter <- iter + 1
    rel.err <- max(abs(beta.new - beta) / abs(beta.new))
    beta <- beta.new
  }
  
  # Calculating std.error and deviance.
  F.inv <- solve(A)
  std.Error <- sqrt(diag(F.inv))
  
  eta = X %*% beta
  deviance = -2 * sum(y*log(pnorm(eta)) + (1 - y)*log(1 -pnorm(eta)))
  
  return (list("coefficients" = data.frame(beta, std.Error),
            "deviance" = deviance,
            "vcov" = F.inv))
}
```


## c)

Simulation of 1000 Bernoulli draws with a random probability.

```{r}
# probability
x = runif(1000, 0, 1)
# draw n bernoulli with prob x
y <- rbinom(1000, 1, x)
df <- data.frame(y, x)
### fit using glm
model <- glm(y ~ x, family = binomial(link = "probit"), data = df)
# beta
model$coefficients
# se for beta
summary(model)
# vcov
vcov(model)
# deviance
model$deviance
### fit using myglm
mymodel <- myglm(y ~ x, data = df)
# beta
mymodel$coefficients
# vcov
mymodel$vcov
# deviance
mymodel$deviance
```



# Problem 2
## a)
 
```{r}
#install.packages("ISwR")
library(ISwR) # Install the package if needed
data(juul)
juul$menarche <- juul$menarche - 1
juul.girl <- subset(juul, age>8 & age<20 & complete.cases(menarche))
```
 
 
```{r}
mod.probit <- glm(menarche ~ age, family=binomial(link="probit"), data= juul.girl)
anova(mod.probit, test = "Chisq")
```

The low p-value suggests that age has an effect on the response variable.

## b)
Relating to the `juul` data set, we define for each observation/individual

$$
y_i = 
\begin{cases}
  0, \text{  if menarche has occured.} \\
  1, \text{  if menarche has not occured.}
\end{cases}
$$
and $t_i$ as the age at the time of examination, which corresponds to `age` in the data set. Let $T_i \sim \mathcal{N}(\mu, \sigma^2)$, where $T_i$ is the time until menarche occurs for the $i$'th individual. Furthermore, let

$$
\begin{split}
\pi_i &:= P(y_i = 1) = P(T_i \le t_i) \\
&= P\left(\frac{T_i -\mu}{\sigma} \le \frac{t_i - \mu}{\sigma}\right) = \Phi\left(\frac{t_i - \mu}{\sigma}\right)
\end{split}
$$
This, in turn, gives
$$
\Phi^{-1}(\pi_i) = -\frac{\mu}{\sigma} + \frac{1}{\sigma}t_i = \beta_0 + \beta_1t_i,
$$
where $\beta_0 = -\mu/\sigma$ and $\beta_1 = 1/\sigma$. Or equivalently:

$$
  \sigma = \frac{1}{\beta_1}, \quad \mu = -\frac{\beta_0}{\beta_1}.
$$
Due to the [ \textcolor{blue}{functional invariance of the maximum likelihood estimator}](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Functional_invariance), we can write the MLEs of $\sigma$ and $\mu$ as 

$$
  \hat\sigma(\hat\beta_1) = \frac{1}{\hat\beta_1}, \quad \hat\mu(\hat\beta_0, \hat\beta_1) = -\frac{\hat\beta_0}{\hat\beta_1},
$$
where $\hat\beta_0$ and $\hat\beta_1$ denote the MLEs of $\beta_0$ and $\beta_1$, respectively. Thus, the maximum likelihood estimates for this data set can be computed as in the code below.

```{r}
mod.probit <- glm(menarche ~ age, family = binomial(link = 'probit'), data = juul.girl)
b <- mod.probit$coefficients
mle.mu <- -b[1]/b[2]
mle.sigma <- 1/b[2] 
```
That is, $\hat{\mu}=$ `r mle.mu` and $\hat{\sigma} =$ `r mle.sigma`. The standard errors (estimates of the standard deviation) of $\hat{\mu}$ and $\hat{\sigma}$ can then be computed using the delta method. A first order Taylor expansion of $\hat{\mu}$ gives

$$
\hat\mu \approx \hat\mu(\boldsymbol{b}) + \nabla \hat\mu(\boldsymbol{b})^T(\hat{\boldsymbol{\beta}} - \boldsymbol{b}),
$$

where we have used the notation $\hat{\boldsymbol{\beta}} = (\hat{\beta}_0, \hat{\beta_1})^T$ and expanded around $\hat{\boldsymbol{\beta}} = \boldsymbol{b} = (b_0,b_1)^T$. Next, we take the variance of this linear approximation, such that

$$
\begin{split}
\mathrm{Var}(\hat\mu) &\approx \mathrm{Var}\left(\nabla\hat{\mu}(\boldsymbol{b})^T\hat{\boldsymbol{\beta}} \right) \\
&=  \nabla\hat{\mu}(\boldsymbol{b})^T \mathrm{Var}(\hat{\boldsymbol{\beta}})\nabla\hat{\mu}(\boldsymbol{b}).
\end{split}
$$
Using $\nabla \hat{\mu} = (-1/\hat\beta_1, \hat{\beta}_0 /\hat{\beta_1}^2)^T$, we can calculate the standard error as in the code below:
```{r}
mod.probit <- glm(menarche ~ age, family = binomial(link = 'probit'), data = juul.girl)
b <- mod.probit$coefficients
grad.mu <- c(-1/b[2], b[1]/b[2]^2)
se.mu <- t(grad.mu) %*% vcov(mod.probit) %*% grad.mu
se.mu
```
That is, $\mathrm{SE}(\hat{\mu}) =$ `r se.mu`. We follow the same procedure to estimate the standard error of $\hat{\sigma}$. First, we approximate it by a first order Taylor series expansion:
$$
\hat\sigma \approx \hat\sigma(\boldsymbol{b}) + \nabla \hat\sigma(\boldsymbol{b})^T(\hat{\boldsymbol{\beta}} - \boldsymbol{b}),
$$
which implies that the variance can be approximated as 

$$
\begin{split}
\mathrm{Var}(\hat\sigma) &\approx \mathrm{Var}\left(\nabla\hat{\sigma}(\boldsymbol{b})^T\hat{\boldsymbol{\beta}} \right) \\
&=  \nabla\hat{\sigma}(\boldsymbol{b})^T \mathrm{Var}(\hat{\boldsymbol{\beta}})\nabla\hat{\sigma}(\boldsymbol{b}).
\end{split}
$$
Using that $\nabla\hat{\sigma} = (0, -1/\beta_1^2)^T$, we calculate the standard error as in the code below.
```{r}
grad.sigma <- c(0, -1/b[2]^2)
se.sigma <- t(grad.sigma) %*% vcov(mod.probit) %*% grad.sigma
se.sigma
```
That is, $\mathrm{SE}(\hat{\sigma}) =$ `r se.sigma`.

## c)
We fit the desired model in R:
```{r}
mod.logit <- glm(menarche ~ age, family = binomial(link = 'logit'), data = juul.girl)
mod.logit$coefficients
```
To find the distribution of the $T_i$'s, we start with the cumulative distribution:

$$
\mathrm{Pr}(T_i \le t_i) = \mathrm{Pr}(y_i = 1 \mid t_i) = \pi_i = \frac{1}{1 + e^{-\eta_i}}.
$$

The pdf of $T_i$ is then given as 

$$
\begin{split}
f_{T_i}(t_i) &= \frac{\mathrm{d}}{\mathrm{d}t_i}\left(\frac{1}{1 + e^{-\eta_i}}\right) = \frac{\beta_1e^{-\beta_0-\beta_1 t_i}}{(1 + e^{-\beta_0-\beta_1 t_i})^2} \\
&= \frac{e^{-(t_i-(-\beta_0/\beta_1))/(1/\beta_1)}}{1/\beta_1(1 + e^{-(t_i-(-\beta_0/\beta_1))/(1/\beta_1)})^2}  = \frac{e^{-(t_i-\mu)/s}}{s(1 + e^{-(t_i-\mu)/s})^2}.
\end{split}
$$

This is the logistic distribution, with parameters $\mu = -\beta_0/\beta_1$ and $s = 1/\beta_1$, where we have used the parametrization from [\textcolor{blue}{Wikipedia}](https://en.wikipedia.org/wiki/Logistic_distribution). We compute estimates of the mean and variance from the estimates of $\beta_0$ and $\beta_1$, which are given in the code output above. We then estimate the mean, given by $\text{E}(T_i) =  -\beta_0/\beta_1 \approx$ `r -mod.logit$coefficients[1]/mod.logit$coefficients[2]`. Next, we estimate the standard deviation, given by  $\sqrt{\text{Var}(T_i)} = s\pi/\sqrt{3} = \pi/(\sqrt{3}\beta_1) \approx$ `r pi/(sqrt(3)*mod.logit$coefficients[2])`.

## d)
We now assume that the latent ages follow a log-normal distribution, i.e.

$$
T_i \sim \mathrm{Lognormal}(\mu, \sigma^2).
$$
This is equivalent to stating that $\ln T_i \sim \mathcal{N}(\mu, \sigma^2)$. Now we can follow the same approach as in 2b):

$$
\begin{split}
\pi_i &:= \mathrm{Pr}(y_i = 1) = \mathrm{Pr}(T_i \le t_i)  = \mathrm{Pr}(\ln T_i \le \ln t_i)\\
&= \mathrm{Pr}\left(\frac{\ln T_i -\mu}{\sigma} \le \frac{\ln t_i - \mu}{\sigma}\right) = \Phi\left(\frac{\ln t_i - \mu}{\sigma}\right)
\end{split}
$$

This, in turn, gives

$$
\Phi^{-1}(\pi_i) = -\frac{\mu}{\sigma} + \frac{1}{\sigma}\ln t_i = \beta_0 + \beta_1 \ln t_i,
$$
where $\beta_0 = -\mu/\sigma$ and $\beta_1 = 1/\sigma$. Consequently, we fit GLM with a probit link-function on $\ln t_i$:

```{r}
mod.lognorm <- glm(menarche ~log(age), family = binomial(link = "probit"), data = juul.girl)
mu.hat <- -mod.lognorm$coefficients[1]/mod.lognorm$coefficients[2]
sigma.hat <- 1/mod.lognorm$coefficients[2]
summary(mod.lognorm)
```

Exactly as in 2b), due to the functional invariance of MLEs, we can estimate the mean of $T_i$ as 
$$
\exp\left(\hat{\mu}+ \frac{\hat{\sigma}^2}{2} \right) = \text{`r exp(mu.hat + sigma.hat^2/2)`},
$$

and we can estimate the standard deviation as 

$$
\sqrt{[\exp(\hat{\sigma}^2) - 1]\exp(2\hat{\mu} + \hat{\sigma}^2) }= \text{`r sqrt((exp(sigma.hat^2) - 1) * (2*mu.hat + sigma.hat^2))`}.
$$

The formulas for mean and standard deviation of the log-normal distribution are gathered from [\textcolor{blue}{Wikipedia}](https://en.wikipedia.org/wiki/Log-normal_distribution).


## e)
The cloglog link function is given by

$$
g(\pi) = \text{cloglog}(\pi) = \ln(-\ln(1-\pi)).
$$
Since our model is assumed to have the form `menarche ~ log(age)` with the cloglog link, $\eta$ becomes
$$
  \eta = \beta_0 + \beta_1 \ln(t),
$$
where $t$ is age. Thus the probability that menarche has occurred is given as
$$
  \pi = g^{-1}(\eta) = 1 - e^{-e^{\eta}} = 1 - e^{-e^{\beta_0 + \beta_1 \ln t}} = 1 - e^{-(e^{\beta_0} t^{\beta_1})},
$$
where it was used that $e^{\beta_1 \ln t} = t^{\beta_1}$. Using that $\pi$ is the cumulative distribution function of $T$, i.e. Pr$(T \le t) = \pi$, it follows that the distribution of $T$ is given by
$$
  T \sim \frac{\partial}{\partial t} \pi = \frac{\partial}{\partial t} \left(1 - e^{-(e^{\beta_0} t^{\beta_1})}\right) = \beta_1 e^{\beta_0}\ t^{\beta_1 - 1}\ e^{-(e^{\beta_0} t^{\beta_1})}.
$$
Recall that the probability density function of a Weibull distribution is given by
$$
    \text{Weibull}(x;\lambda,k) = \frac{k}{\lambda} \left(\frac{x}{\lambda}\right)^{k-1} e^{-(x/\lambda)^k}
$$
where $k$ is the shape parameter and $\lambda$ is the scale parameter. If $k = \beta_1$ and $\lambda = e^{- \frac{\beta_0}{\beta_1}}$, then
$$
\begin{split}
\text{Weibull}\left(t;e^{-\frac{\beta_0}{\beta_1}},\beta_1\right) &= \frac{\beta_1}{e^{-\frac{\beta_0}{\beta_1}}} \left(\frac{x}{e^{-\frac{\beta_0}{\beta_1}}}\right)^{\beta_1-1} e^{-\left(t\  e^{\frac{\beta_0}{\beta_1}}\right)^{\beta_1}}\\
&= \beta_1 e^{\beta_0}\ t^{\beta_1 - 1}\ e^{-(e^{\beta_0} t^{\beta_1})},\\
\end{split}
$$
which shows that $T \sim \text{Weibull}\left(t;e^{-\frac{\beta_0}{\beta_1}},\beta_1\right)$. The shape and scale parameters are given as
$$
k = \beta_1, \qquad \lambda = e^{- \beta_0 / \beta_1}.
$$

Due to the functional invariance of the maximum likelihood estimator, we can write the MLEs of $k$ and $\lambda$ as
$$
\hat k = \hat{\beta_1}, \qquad \hat \lambda = e^{- \hat \beta_0 / \hat \beta_1}.
$$
Fitting the appropriate model in R and finding the estimate for the shape and scale parameter:
```{r}
mod.cloglog <- glm(menarche ~ log(age), family = binomial(link = 'cloglog'), data = juul.girl)

beta0 <- mod.cloglog$coefficients[1]
beta1 <- mod.cloglog$coefficients[2]

k <- beta1
lam <- exp(-beta0/beta1)

k

lam
```
The estimated values for the shape parameter is $k = 13.572$ and the scale parameter is $\lambda = 13.717$.

If $W \sim \text{Weibull}(x;\lambda,k)$, the mean and variance of $W$ are given as
$$
E[\text{W}] = \lambda\ \Gamma\left(1 + \frac{1}{k}\right), \quad \text{Var}[\text{W}] = \lambda^2 \left( \Gamma\left(1 + \frac{2}{k}\right) - \Gamma^2\left(1 + \frac{1}{k}\right) \right),
$$
where $\Gamma$ is the gamma function. In R, we get
```{r}
mu <- lam * gamma(1+1/k)
SE <- lam * sqrt( gamma(1+2/k) - gamma(1+1/k)^2 )

mu

SE
```
The mean is $13.20$ and the standard error is $1.19$.

## f)

Dette må vel også skrives en gang...
