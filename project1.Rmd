---
title: 'TMA4315: Project 1'
author: "Jim Totland, Martin Gudahl Tufte"
date: "9/8/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1
## a)
Since the response variables $y_i \sim \mathrm{Bernoulli}(\pi_i)$, where $\pi_i = \Pr(y_i=1 \mid \boldsymbol{x}_i)$. The conditional mean is given by $\mathrm{E}y_i = \pi_i$, which is connected to the covariates via the following relationship:

$$
\boldsymbol{x}_i^T\boldsymbol{\beta} =: \eta_i = \Phi^{-1}(\pi_i),
$$
or equivalently: $\pi_i = \Phi(\eta_i)$. This results in the likelihood function

$$
\begin{split}
L(\boldsymbol{\beta}) &= \prod_{i= 1}^n \pi_i^{y_i}(1-\pi_i)^{1-y_i} \\
&=\prod_{i = 1}^n\Phi(\eta_i)^{y_i}(1 - \Phi(\eta_i))^{1-y_i}.
\end{split}
$$
Thus, the log-likelihood becomes

$$
l(\boldsymbol{\beta}) := \mathrm{ln}(L(\boldsymbol{\beta})) = \sum_{i = 1}^n \underbrace{y_i \mathrm{ln}(\Phi(\eta_i)) + (1- y_i)\mathrm{ln}(1-\Phi(\eta_i))}_{= l_i(\boldsymbol{\beta})} = \sum_{i = 1}^nl_i(\boldsymbol{\beta}).
$$
To find the score function, we calculate

$$
\begin{split}
  \frac{\partial l_i(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} &= \frac{y_i}{\Phi(\eta_i)} \frac{\partial
  \Phi(\eta_i)}{\partial  \boldsymbol{\beta}} - \frac{1-y_i}{1-\Phi(\eta_i)} \frac{\partial \Phi(\eta_i)}{\partial \boldsymbol{\beta}} \\
  &= \frac{y_i}{\Phi(\eta_i)} \phi(\eta_i)\boldsymbol{x}_i - \frac{1-y_i}{1-\Phi(\eta_i)} \phi(\eta_i)\boldsymbol{x}_i \\
  &= \frac{y_i(1-\Phi(\eta_i)) - (1-y_i)\Phi(\eta_i)}{\Phi(\eta_i)(1-\Phi(\eta_i))}\phi(\eta_i)\boldsymbol{x}_i \\
  &= \frac{y_i - \Phi(\eta_i)}{\Phi(\eta_i)(1-\Phi(\eta_i))}\phi(\eta_i)\boldsymbol{x}_i.
\end{split}
$$
Consequently, the score function is given by 

$$
\boldsymbol{s}(\boldsymbol{\beta} ) = \sum_{i = 1}^n \frac{y_i - \Phi(\eta_i)}{\Phi(\eta_i)(1-\Phi(\eta_i))}\phi(\eta_i)\boldsymbol{x}_i = X^TD\Sigma^{-1}(\boldsymbol{y} - \boldsymbol{\mu}),
$$

where $D = \text{diag}(\phi(\eta_i))$ and $\Sigma = \text{diag}(\text{Var}(y_i)) = \text{diag}(\Phi(\eta_i)(1-\Phi(\eta_i)))$. Next, we find the expected Fisher information, $F(\boldsymbol{\beta})$. We find it by using the result

$$
\begin{split}
  F(\boldsymbol{\beta}) &= \mathrm{Var}(\boldsymbol{s}(\beta)) = \mathrm{Var} \left( \sum_{i = 1}^n \frac{y_i - \Phi(\eta_i)}{\Phi(\eta_i)(1-\Phi(\eta_i))}\phi(\eta_i)\boldsymbol{x}_i \right) \\
  &= \sum_{i = 1}^n \left[ \frac{\phi(\eta_i)}{\Phi(\eta_i)(1 - \Phi(\eta_i))}\right]^2 \mathrm{Var}(y_i \boldsymbol{x}_i) = \sum_{i = 1}^n\left[ \frac{\phi(\eta_i)}{\Phi(\eta_i)(1 - \Phi(\eta_i))}\right]^2  \boldsymbol{x}_i \mathrm{Var}(y_i) \boldsymbol{x}_i^T \\
  &= \sum_{i = 1}^n \left[ \frac{\phi(\eta_i)}{\Phi(\eta_i)(1 - \Phi(\eta_i))}\right]^2  \pi_i(1-\pi_i) \boldsymbol{x}_i \boldsymbol{x}_i^T = \sum_{i = 1}^n \frac{\phi(\eta_i)^2}{\Phi(\eta_i)(1 - \Phi(\eta_i))} \boldsymbol{x}_i \boldsymbol{x}_i^T,
\end{split}
$$
Where in the third equality we have used that the $y_i$'s are independent. The expected Fisher information can also be verified to have this expression by the general relationship
$$
  F(\boldsymbol{\beta}) = \sum_{i=1}^n \frac{h'(\eta_i)^2}{\mathrm{Var}(y_i)} \boldsymbol{x}_i \boldsymbol{x}_i^T,
$$
where $h'(\eta_i) = \Phi'(\eta_i) = \phi(\eta_i)$ and $\mathrm{Var}(y_i) = \pi_i(1-\pi_i) = \Phi(\eta_i)(1-\Phi(\eta_i))$.

 
## b) 

The expected Fisher information is given by

$$
  F(\boldsymbol{\beta}) = \sum_{i = 1}^n \frac{\phi(\eta_i)^2}{\Phi(\eta_i)(1 - \Phi(\eta_i))} \boldsymbol{x}_i \boldsymbol{x}_i^T = X^T W X,
$$
where $W = \mathrm{diag}\left(\frac{\phi(\eta_i)^2}{\Phi(\eta_i)(1 - \Phi(\eta_i))}\right)$.

The Fisher scoring algorithm states that the next iterate is given by

$$
  \boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} + F(\boldsymbol{\beta}^{(t)})^{-1} \boldsymbol{s}(\boldsymbol{\beta}^{(t)}).
$$
<!--
Inserting the expected Fisher information and the score function we get 

$$
  \boldsymbol{\beta}^{(t+1)} = (\boldsymbol{x}^T W^{(t)} \boldsymbol{x})^{-1} \boldsymbol{x}^T W^{(t)} \tilde{\boldsymbol{y}}^{(t)},
$$
where the working response vector $\tilde{\boldsymbol{y}}^{(t)}$ has element $i$ given by

$$
  \tilde{y}_i^{(t)} = \boldsymbol{x}_i^T \boldsymbol{\beta}^{(t)} + \frac{y_i - h(\boldsymbol{x}_i^T \boldsymbol{\beta}^{(t)})}{h'(\boldsymbol{x}_i^T \boldsymbol{\beta}^{(t)})} = \eta_i^{(t)} + \frac{y_i - \Phi(\eta_i^{(t)})}{\phi(\eta_i^{(t)})}.
$$
-->

We also need the deviance, which is defined as

 $$
   D = 2( l_{\mathrm{saturated}} - l(\hat{\boldsymbol{\beta}})).
 $$
When we fit a parameter for each data point (which is the case for the saturated model), the result for the Bernoulli distribution is that $\hat{\pi}_i = y_i$. This means that the likelihood function of the saturated model is given by

$$
L_{\mathrm{saturated}} = \prod_{i = 1}^n \hat{\pi}_i^{y_i}(1-\hat{\pi}_i)^{1-y_i} = \prod_{i = 1}^n y_i^{y_i}(1 + y_i)^{1-y_i} = 1,
$$
Where we have used $0^0 = 1$. Consequently, the log-likelihood $l_{\mathrm{saturated}} = \mathrm{ln}(1) =  0$ and the deviance becomes $-2l(\hat{\boldsymbol{\beta}})$. Next follows the Implementation of `myglm` in R:

```{r}
Phi <- function(x) return (pnorm(x))
phi <- function(x) return (dnorm(x))


myglm <- function(formula, data, start = NULL){
  
  # response variable
  resp <- all.vars(formula)[1]
  y <- as.matrix( data[resp] )
  
  
  # model matrix
  X <- model.matrix(formula, data)
  n <- dim(X)[1]
  p <- dim(X)[2]
  
  
  # starting beta
  if (is.null(start)){
    beta = rep(0, p)
  }
  else {
    beta = start
  }
  
  
  # Fisher scoring algorithm
  max_iter <- 50
  tol <- 1e-10
  iter <- 0
  rel.err <- Inf
  
  F.inv = NULL
  eta = NULL
  
  while (rel.err > tol & iter < max_iter){
    # Calculate eta.
    eta <- X %*% beta
    
    # Calculate score.
    D <- diag(as.vector(phi(eta)), n, n)
    Sigma <- diag(as.vector(Phi(eta)*(1 - Phi(eta))), n, n)
    mu.vec <- as.vector(Phi(eta))
    score = t(X) %*% D %*% solve(Sigma) %*% (y - mu.vec)
    
    # Calculate Fisher information and its inverse.
    W <- diag(as.vector(phi(eta)^2 / (Phi(eta)*(1-Phi(eta)))), n, n)
    F <- t(X) %*% W %*% X
    F.inv <- solve(F)
    
    # Update beta.
    beta.new <- beta + F.inv %*% score
    
    iter <- iter + 1
    rel.err <- max(abs(beta.new - beta) / abs(beta.new))
    beta <- beta.new
  }
  
  # Calculating std.errors and deviance.
  std.Error <- sqrt(diag(F.inv))
  deviance = -2 * sum(y*log(pnorm(eta)) + (1 - y)*log(1 -pnorm(eta)))
  
  return (list("coefficients" = data.frame(beta, std.Error),
            "deviance" = deviance,
            "vcov" = F.inv))
}

```

```{r echo = F, eval = T}
# Uses IRWLS
myglm2 <- function(formula, data, start = NULL){
  
  # response variable
  resp <- all.vars(formula)[1]
  y <- as.matrix( data[resp] )
  
  
  # model matrix
  X <- model.matrix(formula, data)
  n <- dim(X)[1]
  p <- dim(X)[2]
  
  
  # starting beta
  if (is.null(start)){
    beta = rep(0, p)
  }
  else {
    beta = start
  }
  
  
  # Fisher scoring algorithm
  max_iter <- 50
  tol <- 1e-10
  iter <- 0
  rel.err <- Inf
  
  while (rel.err > tol & iter < max_iter){
    # calculate eta, y tilde, W
    eta <- X %*% beta
    y.tilde <- eta + (y - Phi(eta)) / (phi(eta))
    W <- diag( as.vector(phi(eta)^2 / (Phi(eta)*(1-Phi(eta)))), n, n )
    
    
    # update beta
    A <- t(X) %*% W %*% X
    b <- t(X) %*% W %*% y.tilde
    beta.new <- solve(A, b)
    
    iter <- iter + 1
    rel.err <- max(abs(beta.new - beta) / abs(beta.new))
    beta <- beta.new
  }
  
  # Calculating std.error and deviance.
  F.inv <- solve(A)
  std.Error <- sqrt(diag(F.inv))
  
  eta = X %*% beta
  deviance = -2 * sum(y*log(pnorm(eta)) + (1 - y)*log(1 -pnorm(eta)))
  
  return (list("coefficients" = data.frame(beta, std.Error),
            "deviance" = deviance,
            "vcov" = F.inv))
}
```


## c)

Simulation of 1000 Bernoulli draws with a random probability.

```{r}
# probability
x = runif(1000, 0, 1)
# draw n bernoulli with prob x
y <- rbinom(1000, 1, x)
df <- data.frame(y, x)
### fit using glm
model <- glm(y ~ x, family = binomial(link = "probit"), data = df)
# beta
model$coefficients
# se for beta
summary(model)
# vcov
vcov(model)
# deviance
model$deviance
### fit using myglm
mymodel <- myglm(y ~ x, data = df)
# beta
mymodel$coefficients
# vcov
mymodel$vcov
# deviance
mymodel$deviance
```
 
# Problem 2
## a)
 
```{r}
#install.packages("ISwR")
library(ISwR) # Install the package if needed
data(juul)
juul$menarche <- juul$menarche - 1
juul.girl <- subset(juul, age>8 & age<20 & complete.cases(menarche))
```
 
 
```{r}
mod.probit <- glm(menarche ~ age, family=binomial(link="probit"), data= juul.girl)
anova(mod.probit, test = "Chisq")
```

The low p-value suggests that age has an effect on the response variable.

## b)
Relating to the `juul` data set, we define for each observation/individual

$$
y_i = 
\begin{cases}
  0, \text{  if menarche has occured.} \\
  1, \text{  if menarche has not occured.}
\end{cases}
$$
and $t_i$ as the age at the time of examination, which corresponds to `age` in the data set. Let $T_i \sim \mathcal{N}(\mu, \sigma^2)$, where $T_i$ is the time until menarche occurs for the $i$'th individual. Furthermore, let

$$
\begin{split}
\pi_i &:= P(y_i = 1) = P(T_i \le t_i) \\
&= P\left(\frac{T_i -\mu}{\sigma} \le \frac{t_i - \mu}{\sigma}\right) = \Phi\left(\frac{t_i - \mu}{\sigma}\right)
\end{split}
$$
This, in turn, gives
$$
\Phi^{-1}(\pi_i) = -\frac{\mu}{\sigma} + \frac{1}{\sigma}t_i = \beta_0 + \beta_1t_i,
$$
where $\beta_0 = -\mu/\sigma$ and $\beta_1 = 1/\sigma$.


## c)

```{r}
mod.logit <- glm(menarche ~ age, family = binomial(link = 'logit'), data = juul.girl)
mod.logit$coefficients[2]

```
To show find the distribution of the $T_i$'s, we start with the cumulative distribution:

$$
\mathrm{Pr}(T_i \le t_i) = \mathrm{Pr}(y_i = 1 \mid t_i) = \pi_i = \frac{1}{1 + e^{-\eta_i}}.
$$

The pdf of $T_i$ is then given as 

$$
\begin{split}
f_{T_i}(t_i) &= \frac{\mathrm{d}}{\mathrm{d}t_i}\left(\frac{1}{1 + e^{-\eta_i}}\right) = \frac{\beta_1e^{-\beta_0-\beta_1 t_i}}{(1 + e^{-\beta_0-\beta_1 t_i})^2} \\
&= \frac{e^{-(t_i-(-\beta_0/\beta_1))/(1/\beta_1)}}{1/\beta_1(1 + e^{-(t_i-(-\beta_0/\beta_1))/(1/\beta_1)})^2}  = \frac{e^{-(t_i-\mu)/s}}{s(1 + e^{-(t_i-\mu)/s})^2}.
\end{split}
$$

This is the logistic distribution, with parameters $\mu = -\beta_0/\beta_1$ and $s = 1/\beta_1$, where we have used the parametrization from [\textcolor{blue}{Wikipedia}](https://en.wikipedia.org/wiki/Logistic_distribution). We compute estimates of the mean and variance from the estimates of $\beta_0$ and $\beta_1$ in the output above. An estimate of the mean is then given by $\text{E}(T_i) =  -\beta_0/\beta1 \approx$ `r -mod.logit$coefficients[1]/mod.logit$coefficients[2]` and an estimate of the variance is given by $\text{Var}(T_i) = s^2\pi^2/3 = \pi^2/(3\beta_1^2) \approx$ `r pi^2/(3*mod.logit$coefficients[2]^2)`.

## d)
We now assume that the latent ages follow a log-normal distribution, i.e.

$$
T_i \sim \mathrm{Lognormal}(\mu, \sigma^2).
$$
This is equivalent to stating that $\ln T_i \sim \mathcal{N}(\mu, \sigma^2)$. Now we can follow the same approach as in 2b):

$$
\begin{split}
\pi_i &:= \mathrm{Pr}(y_i = 1) = \mathrm{Pr}(T_i \le t_i)  = \mathrm{Pr}(\ln T_i \le \ln t_i)\\
&= \mathrm{Pr}\left(\frac{\ln T_i -\mu}{\sigma} \le \frac{\ln t_i - \mu}{\sigma}\right) = \Phi\left(\frac{\ln t_i - \mu}{\sigma}\right)
\end{split}
$$

This, in turn, gives

$$
\Phi^{-1}(\pi_i) = -\frac{\mu}{\sigma} + \frac{1}{\sigma}t_i = \beta_0 + \beta_1 \ln t_i,
$$
where $\beta_0 = -\mu/\sigma$ and $\beta_1 = 1/\sigma$. Consequently, we fit GLM with a probit link-function and...

