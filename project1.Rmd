---
title: 'TMA4315: Project 1'
author: "Jim Totland, Martin Gudahl Tufte"
date: "9/8/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1
## a)
Since the response variables $y_i \sim \mathrm{Bernoulli}(\pi_i)$, where $\pi_i = \Pr(y_i=1 \mid \boldsymbol{x}_i) = \Phi(\boldsymbol{x}_i^T \boldsymbol{\beta})$, the conditional mean is given by $\mathrm{E}y_i = \pi_i$, which is connected to the covariates via the following relationship:

$$
\boldsymbol{x}_i^T\boldsymbol{\beta} =: \eta_i = \Phi^{-1}(\pi_i),
$$
which implies that $\pi_i = \Phi(\eta_i)$. This results in the likelihood function

$$
\begin{split}
L(\boldsymbol{\beta}) &= \prod_{i= 1}^n \pi_i^{y_i}(1-\pi_i)^{1-y_i} \\
&=\prod_{i = 1}^n\Phi(\eta_i)^{y_i}(1 - \Phi(\eta_i))^{1-y_i}.
\end{split}
$$
Thus, the log-likelihood becomes

$$
l(\boldsymbol{\beta}) := \mathrm{ln}(L(\boldsymbol{\beta})) = \sum_{i = 1}^n y_i \mathrm{ln}(\Phi(\eta_i)) + (1- y_i)\mathrm{ln}(1-\Phi(\eta_i)) = \sum_{i = 1}^nl_i(\boldsymbol{\beta}).
$$
To find the score function, we calculate

$$
\begin{split}
  \frac{\partial l_i(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}} &= \frac{y_i}{\Phi(\eta_i)} \frac{\partial
  \Phi(\eta_i)}{\partial  \boldsymbol{\beta}} - \frac{1-y_i}{1-\Phi(\eta_i)} \frac{\partial \Phi(\eta_i)}{\partial \boldsymbol{\beta}} \\
  &= \frac{y_i}{\Phi(\eta_i)} \phi(\eta_i)\boldsymbol{x}_i - \frac{1-y_i}{1-\Phi(\eta_i)} \phi(\eta_i)\boldsymbol{x}_i \\
  &= \frac{y_i(1-\Phi(\eta_i)) - (1-y_i)\Phi(\eta_i)}{\Phi(\eta_i)(1-\Phi(\eta_i))}\phi(\eta_i)\boldsymbol{x}_i \\
  &= \frac{y_i - \Phi(\eta_i)}{\Phi(\eta_i)(1-\Phi(\eta_i))}\phi(\eta_i)\boldsymbol{x}_i.
\end{split}
$$
Consequently, the score function is given by 

$$
\boldsymbol{s}(\boldsymbol{\beta} ) = \sum_{i = 1}^n \frac{y_i - \Phi(\eta_i)}{\Phi(\eta_i)(1-\Phi(\eta_i))}\phi(\eta_i)\boldsymbol{x}_i.
$$

Next, we find the expected Fisher information, $F(\boldsymbol{\beta})$. We find it by using the result

$$
\begin{split}
  F(\boldsymbol{\beta}) &= \mathrm{Var}(\boldsymbol{s}(\beta)) = \mathrm{Var} \left( \sum_{i = 1}^n \frac{y_i - \Phi(\eta_i)}{\Phi(\eta_i)(1-\Phi(\eta_i))}\phi(\eta_i)\boldsymbol{x}_i \right) \\
  &= \sum_{i = 1}^n \left[ \frac{\phi(\eta_i)}{\Phi(\eta_i)(1 - \Phi(\eta_i))}\right]^2 \mathrm{Var}(y_i \boldsymbol{x}_i) = \sum_{i = 1}^n\left[ \frac{\phi(\eta_i)}{\Phi(\eta_i)(1 - \Phi(\eta_i))}\right]^2  \boldsymbol{x}_i \mathrm{Var}(y_i) \boldsymbol{x}_i^T \\
  &= \sum_{i = 1}^n \left[ \frac{\phi(\eta_i)}{\Phi(\eta_i)(1 - \Phi(\eta_i))}\right]^2  \pi_i(1-\pi_i) \boldsymbol{x}_i \boldsymbol{x}_i^T = \sum_{i = 1}^n \frac{\phi(\eta_i)^2}{\Phi(\eta_i)(1 - \Phi(\eta_i))} \boldsymbol{x}_i \boldsymbol{x}_i^T,
\end{split}
$$
Where in the third equality we have used that the $y_i$'s are independent. The expected Fisher information can also be verified to have this expression by the relationship
$$
  F(\beta) = \sum_{i=1}^n \frac{h'(\eta_i)^2}{\mathrm{Var}(y_i)} \boldsymbol{x}_i \boldsymbol{x}_i^T,
$$
where $h'(\eta_i) = \Phi'(\eta_i) = \phi(\eta_i)$ and $\mathrm{Var}(y_i) = \pi_i(1-\pi_i) = \Phi(\eta_i)(1-\Phi(\eta_i))$.

 
## b) The expected Fisher information is given by

$$
  F(\beta) = \sum_{i = 1}^n \frac{\phi(\eta_i)^2}{\Phi(\eta_i)(1 - \Phi(\eta_i))} \boldsymbol{x}_i \boldsymbol{x}_i^T = \boldsymbol{x}^T W \boldsymbol{x},
$$
where $W = \mathrm{diag}\left(\frac{\phi(\eta_i)^2}{\Phi(\eta_i)(1 - \Phi(\eta_i))}\right)$.

The Fisher scoring algorithm states that the next iterate is given by

$$
  \boldsymbol{\beta}^{(t+1)} = \boldsymbol{\beta}^{(t)} + F(\boldsymbol{\beta}^{(t)})^{-1} \boldsymbol{s}(\boldsymbol{\beta}^{(t)}).
$$
Inserting the expected Fisher information and the score function we get

$$
  \boldsymbol{\beta}^{(t+1)} = (\boldsymbol{x}^T W^{(t)} \boldsymbol{x})^{-1} \boldsymbol{x}^T W^{(t)} \tilde{\boldsymbol{y}}^{(t)},
$$
where the working response vector $\tilde{\boldsymbol{y}}^{(t)}$ has element $i$ given by

$$
  \tilde{y}_i^{(t)} = \boldsymbol{x}_i^T \boldsymbol{\beta}^{(t)} + \frac{y_i - h(\boldsymbol{x}_i^T \boldsymbol{\beta}^{(t)})}{h'(\boldsymbol{x}_i^T \boldsymbol{\beta}^{(t)})} = \eta_i^{(t)} + \frac{y_i - \Phi(\eta_i^{(t)})}{\phi(\eta_i^{(t)})}.
$$
Implementing `myglm` in R:

```{r}
Phi <- function(x) return (pnorm(x))
phi <- function(x) return (dnorm(x))


myglm <- function(formula, data, start = NULL){
  
  # response variable
  resp <- all.vars(formula)[1]
  y <- as.matrix( data[resp] )
  
  
  # model matrix
  X <- model.matrix(formula, data)
  n <- dim(X)[1]
  p <- dim(X)[2]
  
  
  # starting beta
  if (is.null(start)){
    beta = rep(0, p)
  }
  else {
    beta = start
  }
  
  
  # Fisher scoring algorithm
  max_iter <- 50
  tol <- 1e-10
  iter <- 0
  rel.err <- Inf
  
  while (rel.err > tol & iter < max_iter){
    # calculate eta, y tilde, W
    eta <- X %*% beta
    y.tilde <- eta + (y - Phi(eta)) / (phi(eta))
    W <- diag( as.vector(phi(eta)^2 / (Phi(eta)*(1-Phi(eta)))), n, n )
    
    
    # update beta
    A <- t(X) %*% W %*% X
    b <- t(X) %*% W %*% y.tilde
    beta.new <- solve(A, b)
    
    iter <- iter + 1
    rel.err <- max(abs(beta.new - beta) / abs(beta.new))
    beta <- beta.new
  }
  
  
  
  
  # remains to find the coefficients matrix, deviance and estimated variance matrix
  
  coeff <- 1
  deviance <- 1
  vcov <- 1
  
  return (beta)
}

#beta <- myglm(menarche ~ age, juul.girl)

#beta

#X <- model.matrix(menarche ~ age, juul.girl)

```

## c)

```{r}

# probability
x = runif(1000, 0, 1)
# draw n bernoulli with prob x
y <- rbinom(1000, 1, x)

df <- data.frame(y, x)



### fit using glm
model <- glm(y ~ poly(x,2), family = binomial(link = "probit"), data = df)

# beta
model$coefficients

# vcov
vcov(model)

# deviance
# ...

### fit using myglm
beta <- myglm(y ~ poly(x,2), data = df)

# beta
t(beta)

# vcov
# ...

# deviance
# ...

```




 
 
# Problem 2
## a)
 
```{r}
#install.packages("ISwR")
library(ISwR) # Install the package if needed
data(juul)
juul$menarche <- juul$menarche - 1
juul.girl <- subset(juul, age>8 & age<20 & complete.cases(menarche))
```
 
 
```{r}
?juul
head(juul.girl)
model <- glm(menarche ~ age, family=binomial(link="probit"), data= juul.girl)
anova(model, test = "Chisq")
```

The low p-value suggests that age has an effect on the response variable.

## b)
Relating to the `juul` data set, we define for each observation/individual

$$
y_i = 
\begin{cases}
  0, \text{  if menarche has occured.} \\
  1, \text{  if menarche has not occured.}
\end{cases}
$$
and $t_i$ as the age at the time of examination, which corresponds to `age` in the data set. Let $T_i \sim N(\mu, \sigma)$, where $T_i$ is the time until menarche occurs for the $i$'th individual. Furthermore, let

$$
\begin{split}
\pi_i &:= P(y_i = 1) = P(T_i \le t_i) \\
&= P\left(\frac{T_i -\mu}{\sigma} \le \frac{t_i - \mu}{\sigma}\right) = \Phi\left(\frac{t_i - \mu}{\sigma}\right)
\end{split}
$$
This, in turn, gives
$$
\Phi^{-1}(\pi_i) = -\frac{\mu}{\sigma} + \frac{1}{\sigma}t_i = \beta_0 + \beta_1t_i,
$$
where $\beta_0 = -\mu/\sigma$ and $\beta_1 = 1/\sigma$.
