---
title: 'TMA4315: Project 3'
author: "jototlan@stud.ntnu.no (10018), martigtu@stud.ntnu.no (10037)"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
long <- read.csv("https://www.math.ntnu.no/emner/TMA4315/2020h/eliteserie.csv", 
                 colClasses = c("factor","factor","factor","numeric"))
head(long)
```
# a)
We consider the model
```{r}
library(glmmTMB)
mod <- glmmTMB(goals ~ home + (1|attack) + (1|defence),  poisson, data=long, REML=TRUE)
```

## Model and Notation
We use a triple index to denote the response, $y_{ijk}$, where $i$ is the attacking team, $j$ is the defending team, and $k \in \{0, 1\}$ where $k=1$ indicates that the attacking team, $i$, is playing home, while $k = 0$ indicates that the attacking team is playing away. Clearly, $i,j \in \{1,2, \ldots 16 \}$ and $i \ne j$. The distributional assumption on the response is $y_{ijk}|\gamma_i^a,\gamma_j^d \sim \mathrm{Poisson}(\lambda_{ijk})$. Here, $\gamma_i^a$ is the effect of team $i$ attacking an $\gamma_j^d$ is the effect of team $j$ defending. The conditional mean is connected to the covariates by the canonical link function,

$$
\lambda_{ijk} = \exp\left(\beta_0 + \beta_h k + \gamma_{i}^a + \gamma_{j}^{d} \right) = \exp(\eta_{ijk}),
$$

where $\beta_0$ is the intercept and $\beta_h$ is the effect of playing home. The random effects are independent and identically distributed, such that

$$
\gamma_{i}^a \overset{iid}\sim \mathcal{N}(0, \tau_{a}^2) \quad \text{and} \quad \gamma_{j}^d \overset{iid}\sim \mathcal{N}(0, \tau_{d}^2).
$$
We also assume that $\gamma_i^a$ and $\gamma_j^d$ are independent $\forall i,j$. The model parameters are computed with a restricted maximum likelihood approach (REML).

## Distributional Assumption on Response

Assuming that the response follows a Poisson distribution amounts to making the following assumptions: 
\begin{enumerate}
\item Goals are scored independently, i.e. the number of goals scored within disjoint time intervals is independent.
\item The number of goals scored in a time interval is proportional to the length of the interval.
\item Two (or more) goals cannot be scored at exactly the same instance.
\end{enumerate}

The last two assumptions seem very reasonable; two goals can obviously not occur at the same time, and more time gives more opportunities for goal scoring. The first one, however, is more questionable. For example, a team which has just conceded a goal close to the end of the game might play more aggressively to salvage a draw, hence increasing the likelihood of more goals being scored. Despite this, the Poisson distribution seems like a reasonable choice to model this process.


# b)
We print out the summary and random effects:
```{r}
sum <- summary(mod)
sum
beta.0 <- sum$coefficients$cond[1]
beta.h <- sum$coefficients$cond[2]

rf <- ranef(mod)
rf
```

The effect of playing home is positive and statistically significant. According to the output, the team playing home scores $\exp(0.40716)=$ `r exp(0.40716)` more goals on average, i.e. about 50% more goals on average. This seems reasonable from an intuitive perspective. Looking at the estimated random effects, we can e.g. consider $\gamma_\text{Rosenborg}^{d} \approx -0.153$. This is the lowest value among all teams, which indicates that Rosenborg is the best defending team. To check this, we calculate the average number of goals conceded per match by each team:
```{r}
no.NA = long[!is.na(long$goals), c("defence", "goals")]
agg = aggregate(no.NA$goals, by = list(no.NA$defence), FUN = mean)
colnames(agg) <- c("Team", "Avg. # of conceded goals")
sorted <- agg[order(agg[,2]), ]
knitr::kable(sorted)
```
As expected, Rosenborg has the lowest average number of conceded goals. Brann comes second, which is also reflected in their defence strength. We do the same analysis with respect to the average number of goals scored in each game:
```{r}
no.NA = long[!is.na(long$goals), c("attack", "goals")]
agg = aggregate(no.NA$goals, by = list(no.NA$attack), FUN = mean)
colnames(agg) <- c("Team", "Avg. # of goals scored")
sorted <- agg[order(-agg[,2]), ]
knitr::kable(sorted)
```
We observe that Molde scores the most goals on average, which is reflected in their attacking strength. Next comes Rosenborg, etc. We conclude that the parameter estimates seem reasonable.

From the model assumptions, namely $\gamma_{i}^a \sim \mathcal{N}(0, \tau_{a}^2)$ and $\gamma_{j}^d \sim \mathcal{N}(0, \tau_{d}^2)$, the average attack strength and the average defense strength are equal to 0. An estimate of the expected number of goals which the average attacking team scores is thus given as

$$
\exp(\beta_0 + \beta_h) \approx \text{`r exp(beta.0 + beta.h)`}.
$$
Since the the number of goals scored is assumed to follow a Poisson distribution, an estimate of the variance is given by the same value. We now consider the the situation when the average defense team is attacking (here we assume that they also have average attack strength and that the average attack team has average defense). Then, an estimate of the number of goals scored is given as

$$
\exp(\beta_0) \approx \text{`r exp(beta.0)`}.
$$
Again, since we consider a Poisson-distributed variable, an estimate of the variance i given by the same value.

# c)

The expected value and variance of goals scored by two randomly selected teams can be found by the laws of total expectation and total variance conditioned on the random effects given by the two teams. For this, we use that the distribution $\text{Lognormal}(0,\tau^2)$ has mean $\exp(\tau^2/2)$ and variance $(\exp(\tau^2)-1)\exp(\tau^2)$.

The law of total expectation gives
$$
\begin{split}
  \text{E}[y_{ijk}] &= \text{E}\left[\text{E}\left[y_{ijk} \mid \gamma_{i}^a, \gamma_{j}^d\right]\right]\\
  &= \text{E}\left[ \text{exp}\left(\beta_0 + \beta_h k + \gamma_{i}^a + \gamma_{j}^d\right) \right]\\
  &= \text{exp}\left( \beta_0 + \beta_h k\right) \text{E}\left[\exp\left(\gamma_{i}^a + \gamma_{j}^d\right)\right] \\
  &= \text{exp}\left( \beta_0 + \beta_h k\right) \text{exp}\left( \frac{\tau_a^2+\tau_d^2}{2}\right). \\
\end{split}
$$

The law of total variance gives
$$
\begin{split}
  \text{Var}[y_{ijk}] &= \text{E}\left[\text{Var}\left[y_{ijk} \mid \gamma_{i}^a, \gamma_{j}^d\right]\right] + \text{Var}\left[\text{E}\left[y_{ijk} \mid \gamma_{i}^a, \gamma_{j}^d\right]\right]\\
  &= \text{E}\left[ \text{exp}\left(\beta_0 + \beta_h k + \gamma_{i}^a + \gamma_{j}^d\right) \right] + \text{Var}\left[ \text{exp}\left(\beta_0 + \beta_h k + \gamma_{i}^a + \gamma_{j}^d\right) \right]\\
  &= \text{E}[y_{ijk}]  + \text{exp}\left( 2(\beta_0 + \beta_h k\right)) \text{Var}\left[ \text{exp}\left(\gamma_{i}^a + \gamma_{j}^d\right) \right]\\
  &= \text{exp}\left( \beta_0 + \beta_h k\right) \text{exp}\left( \frac{\tau_a^2+\tau_d^2}{2}\right) + \text{exp}\left( 2\left(\beta_0 + \beta_h k\right)\right) \left( \text{exp}\left( \tau_a^2+\tau_d^2\right) - 1 \right) \text{exp}\left( \tau_a^2+\tau_d^2\right).\\
\end{split}
$$
Note that the total marginal variance consists of two terms, given as 
$$
  \text{Var}[y_{ijk}] = \underbrace{\text{E}\left[\text{Var}\left[y_{ijk} \mid \gamma_{i}^a, \gamma_{j}^d\right]\right]}_{\text{Variance of the game}} + \underbrace{\text{Var}\left[\text{E}\left[y_{ijk} \mid \gamma_{i}^a, \gamma_{j}^d\right]\right]}_{\text{Varaince of team strengths}}.
$$
The first term is the expected variance of goals, which is a measure of the inherent randomness of the football game given the two teams. The second term is the variance of the expected goals, which is attributed to the variance in the strength of the two teams, since the expected goals are constant given two teams, so the randomness comes from the difference in strengths among the teams. We now calculate the proportion of variance explained by the two terms without and with home field advantage.

## Estimating the proportions with no home field advantage:
```{r}
# Parameters from model
beta <- summary(mod)$coefficients$cond[,1]
beta0 = beta[1]
beta.h = beta[2]

# Variance of random effects
tau2.attack <- summary(mod)$var$cond$attack[1]
tau2.defence <- summary(mod)$var$cond$defence[1]
tau2 <- tau2.attack + tau2.defence

# Marginal variance
var0.game <- exp(beta0 + tau2/2)
var0.strength <- exp(2*beta0)*(exp(tau2)-1)*exp(tau2)
var0 <- var0.game + var0.strength

# Proportion of variance
prop0.game = var0.game/var0
prop0.strength = var0.strength/var0
```
With no home field advantage for the attacking team, the total marginal variance of the number of goals is `r var0`, where the inherent randomness of the game accounts for `r round(prop0.game, 4)` and the variation in team strengths account for `r round(prop0.strength, 4)`.

## Estimating the proportions with home field advantage:
```{r}
# Marginal variance
var1.game <- exp(beta0 + beta.h + tau2/2)
var1.strength <- exp(2*(beta0 + beta.h))*(exp(tau2)-1)*exp(tau2)
var1 <- var1.game + var1.strength

# Proportions of variance
prop1.game = var1.game / var1
prop1.strength = var1.strength / var1
```
If the attacking team has a home field advantage, the total marginal variance of the number of goals is `r var1`, where the inherent randomness of the game accounts for `r round(prop1.game, 4)` and the variation in team strengths account for `r round(prop1.strength, 4)`. We note that also here the majority of variance can be explained by the game itself.

An interesting observation is that when the home field advantage is present, the proportion of variance explained by the strengths of the teams is higher. This follows directly from the equation of the total marginal variance when $\beta_h > 0$, as the second term is multiplied by an extra factor of $\exp(\beta_h) > 1$.


# d)

We want to test if the two random effects in the model are significant. Since $\gamma^a \sim \mathcal{N}(0, \tau^2_a)$ and $\gamma^d \sim \mathcal{N}(0, \tau^2_d)$, this is equivalent to testing whether the variance of each random effect is positive. The hypothesis test for determining if the effect of attacking is significant can be formulated as
$$
  H_0 : \tau^2_a = 0 \quad \text{vs.} \quad H_1 : \tau^2_a > 0,
$$
and a similar test can be constructed for the effect of defending.

This can be carried out using a likelihood-ratio test. The test statistic is $\lambda_{LRT} := -2(l_0-l_1)$, where $l_0$ and $l_1$ are the log-likelihoods of the under $H_0$ and $H_1$, respectively. We cannot apply Wilks' theorem directly here, as standard asymptotic theory is violated. The reason for this is that under $H_0$, there is a 50% chance that the MLE of $\tau_a$ under $H_1$ falls on the boundary, such that the corresponding LRT statistic takes a value of 0. This is due to the expected value of the score function $s(\tau^2_a) := \frac{\partial l_1}{\partial \tau_a^2}$ evaluated at zero is zero, $\mathrm{E}\left[ s(0) \right] = 0$, implying that whenever the slope of $s(0)$ is negative, the maximum likelihood estimator is zero. This results in a mixture of two distributions
$$
  \lambda_{LRT} \sim \frac12\chi^2_0 : \frac12 \chi^2_1
$$
where $\chi^2_0$ is simply the point mass distribution located at zero. The critical value $C$ of the likelihood-ratio test is given as
$$
  P(\lambda_{LRT} \ge C) = \frac12 P(\chi^2_0 \ge C) + \frac12 P(\chi^2_1 \ge C) = \alpha.
$$
Since $C > 0$ for $\alpha < \frac12$ the first term vanishes, so $C$ is the upper $2\alpha$ quantile of a chi-squared distribution with one degree of freedom. Using $\alpha = 0.05$ we get the critical value
```{r}
alpha <- 0.05
qchisq(1-2*alpha, df=1)
```

## Testing significance of effect of attack

```{r}
mod.no_attack <- glmmTMB(goals ~ home + (1|defence), poisson, data=long, REML=TRUE)
LRT.no_attack <- -2*as.numeric(logLik(mod.no_attack) - logLik(mod))

p.value <- 0.5 * pchisq(LRT.no_attack, df=0, lower.tail = FALSE) +
           0.5 * pchisq(LRT.no_attack, df=1, lower.tail = FALSE)
p.value
```
The $p$-value suggest that we can not reject $H_0$, meaning that the effect of attack is not significant. The test statistic has value `r LRT.no_attack`, which is lower than the critical value.

## Testing significance of effect of defence
We now consider
$$
  H_0 : \tau^2_d = 0 \quad \text{vs.} \quad H_1 : \tau^2_d > 0,
$$
which results in a similar test:
```{r}
mod.no_defence <- glmmTMB(goals ~ home + (1|attack), poisson, data=long, REML=TRUE)
LRT.no_defence <- -2*as.numeric(logLik(mod.no_defence) - logLik(mod))

p.value <- 0.5 * pchisq(LRT.no_defence, df=0, lower.tail = FALSE) +
           0.5 * pchisq(LRT.no_defence, df=1, lower.tail = FALSE)
p.value
```
The $p$-value suggest that the effect of defence is not significant. The test statistic has value `r LRT.no_defence`, which is again lower than the critical value.

## Testing the effect of home field advantage

Our hypothesis can be formulated as
$$
  H_0 : \beta_h = 0 \quad \text{vs.} \quad H_1 : \beta_h \ne 0,
$$
where under $H_1$, the home field advantage could be either negative or positive.

The restricted maximum likelihood (REML) is often preferred when fitting generalized mixed models as they often produce less biased estimators\ \textcolor{red}{??} than using normal maximum likelihood estimation. However, this is not possible here, as REML uses a likelihood function on a transformed set of data, so that nuisance parameters have no effect. Under $H_0$, this is simply the intercept $\beta_0$. But under $H_1$, we also have the parameter $\beta_h$ for the home field advantage. This means that the two models lead to different means using REML, making the likelihoods non-comparable. Setting `REML = FALSE` avoids this issue and fits the models using normal maximum likelihoods.


```{r}
mod.ML <- glmmTMB(goals ~ home + (1|attack) + (1|defence), poisson, data=long, REML=FALSE)
mod.no_home <- glmmTMB(goals ~ (1|attack) + (1|defence), poisson, data=long, REML=FALSE)

LRT.no_home <- -2*as.numeric(logLik(mod.no_home) - logLik(mod.ML))

p.value <- pchisq(LRT.no_home, df=1, lower.tail = FALSE)
p.value
```
The low $p$-value is smaller than the significance level, indicating that the effect of home field is significant.


# e)

```{r}
frankv <- data.table::frankv

ranking <- function(df){
  n.teams <- length(unique(df$attack))
  stats <- data.frame(row.names = unique(df$attack), points = rep(0, n.teams),
                      goal.diff = rep(0, n.teams), goals.scored = rep(0, n.teams))
  
  for(n in seq(1, dim(df)[1], 2)){
    team1 <- as.character(df$attack[n])
    team2 <- as.character(df$defence[n])
    goals1 = df$goals[n]
    goals2 = df$goals[n+1]
    
    # skip missing values
    if (is.na(goals1) | is.na(goals2)){
      next
    }
    else if(goals1 > goals2){
      stats[team1,"points"] = stats[team1,"points"] + 3
    }
    else if(goals2 > goals1){
      stats[team2,"points"] = stats[team2,"points"] + 3
    }
    else {
      stats[team1,"points"] = stats[team1,"points"] + 1
      stats[team2,"points"] = stats[team2,"points"] + 1
    }
    stats[team1,"goal.diff"] = stats[team1,"goal.diff"] + goals1 - goals2
    stats[team2,"goal.diff"] = stats[team2,"goal.diff"] + goals2 - goals1
    stats[team1,"goals.scored"] = stats[team1,"goals.scored"] + goals1
    stats[team2,"goals.scored"] = stats[team2,"goals.scored"] + goals2
  }
  stats$rank <- frankv(stats, cols=c("points","goal.diff","goals.scored"),
                       order=-1, ties.method="random")
  stats
}

stats <- ranking(long)
knitr::kable(stats[order(stats$rank),])
```

# f)

We simulate 1000 realization of the whole series by using the predicted mean $\hat \mu := \hat \lambda$ from the model created in a).

```{r, cache=TRUE}
set.seed(0)
# number of matches
n = dim(long)[1]

# predicted mean goals scored in each match
mu.hat <- predict(mod, newdata=long, type="response")

# matrix to store all simulated rankings
rank.matrix <- matrix(0, nrow = 1000, ncol = 16)
colnames(rank.matrix) = unique(long$attack)

# doing the simulations
temp <- long$goals
for(i in 1:1000){
  long$goals <- rpois(n, mu.hat)
  rank.matrix[i,] <- ranking(long)$rank
}
long$goals <- temp

# probabilities of each observation
probabilities <- apply(rank.matrix, 2, table)/1000

# summary with mean ranking and probabilities
rank.summary <- data.frame(row.names = unique(long$attack),
                           mean = colMeans(rank.matrix),
                           P = t(probabilities))

# print the summary
output <- rank.summary[order(rank.summary$mean),]
library(kableExtra)
knitr::kable(output)
```

The results of the simulations are summarized on the previous page, where we have included the ranking probabilities of each team and the mean ranking. Sorting the teams by their average ranking in ascending order, we see that Rosenborg has the lowest mean ranking, as is expected from the rankings obtained in e).

# g)


We start by creating a data frame with the rankings from f) and the corresponding random effects, as well as the difference between attack strength and defense strength.
```{r}
re <- ranef(mod)

# Create new data frame for rankings and random effects.
df <- data.frame("ranking" = 1:16)
row.names(df) <- row.names(output)
df$attack <- re$cond$attack[row.names(df), ]
df$defence <- re$cond$defence[row.names(df), ]
df$difference = df$attack - df$defence
df
```

Next, we do some plotting:
```{r}
plot(df$ranking, df$attack)
plot(df$ranking, df$defence)
plot(df$ranking, df$difference)
```


From the plots, there seems to be a linear relationship between both the random effects and the simulated ranking. The linear relationship is clearly visible when we plot the difference against the ranking.




