---
title: 'TMA4315: Project 2'
author: "jototlan@stud.ntnu.no (10018), martigtu@stud.ntnu.no (10037)"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1
```{r}
mammals <- read.table(
  "https://www.math.ntnu.no/~jarlet/statmod/mammals.dat",
  header=T)
```

## a)
```{r}
plot(log(mammals$body), log(mammals$brain))
```


The log-log plot of the brain mass against body mass seems to reveal a linear trend. We thus fit the following model:

```{r}
mod0 <- lm(log(brain) ~ log(body), data = mammals)
summary(mod0)
```

If we let $\boldsymbol{y} = [y_1, \ldots, y_n]^T$ denote the brain mass and $\boldsymbol{x}_b = [x_{b1}, \ldots, x_{bn}]^T$ denote the corresponding body mass, we have fitted the model $\ln(y_i) = \beta_0 + \beta_1 \ln(x_{bi}))$, $i = 1,2\ldots n$, with parameter estimates given in the summary above.

## b)
The extended model is fitted below.
```{r}
mammals$is.human = as.factor(mammals$species == "Human")

mod1 <- lm(log(brain) ~ log(body) + is.human, data = mammals)
summary(mod1)
```
Let $\hat{\boldsymbol{\beta}} = \begin{bmatrix}\hat{\beta}_0 & \hat{\beta}_1 & \hat{\beta}_2\end{bmatrix}^T$ be the coefficient estimates given in the summary above, where $\hat{\beta}_2 \approx$ `r mod1$coefficients[3]` models the effect which being a human has on the (log of) brain size. Since we have used a log-transform on both the brain mass and body mass, humans will according to the model be larger by a factor of $e^{\hat{\beta}_2} =$ `r exp(mod1$coefficients[3])`.

We use the notation $\boldsymbol{y} = X\boldsymbol{\beta} + \boldsymbol{\varepsilon}$ to represent the linear model. Here, $X$ is the $n \times p$ design matrix, where $n$ is the number of observations and $p$ is the number of parameters used in the model. Here, $X = \begin{bmatrix}\boldsymbol{1} & \ln\boldsymbol{x}_b & \boldsymbol{x}_h\end{bmatrix}$, where $\boldsymbol{x}_h = [x_{h1}, \ldots, x_{hn}]^T = [0, \ldots 0, 1, 0, \ldots, 0]^T$ which has a nonzero entry $x_{hh} = 1$ only for humans only. As usual, $\boldsymbol{\varepsilon} \sim \mathcal{N}(\boldsymbol{0}, \sigma^2 I_n)$. We know that

$$
\hat{\boldsymbol{\beta}} \sim \mathcal{N}(\boldsymbol{\beta}, \sigma^2(X^TX)^{-1}).
$$
Now we want to perform the hypothesis test

$$
H_0: \beta_2 = 0 \quad \text{vs.} \quad H_1: \beta_2 > 0.
$$
Under $H_0$, we obtain that (we also index from 0 in the design matrix)

$$
\frac{\hat{\beta}_2}{\sigma\sqrt{(X^TX)^{-1}_{2,2}}} \sim \mathcal{N}(0, 1).
$$

Combining this with the fact that 
$$
\frac{(n-p)s^2}{\sigma^2} \sim \chi^2_{n-p},
$$
where $s^2 = SSE/(n-p)$, we obtain the test statistic

$$
T_1 = \frac{\hat{\beta}_2}{s\sqrt{(X^TX)^{-1}_{2,2}}} \sim t_{n-p},
$$
under $H_0$. We perform the calculations in R:
```{r}
n <- nrow(mammals)
p <- 3
beta.2.hat <- mod1$coefficients[3]
s <- sqrt(deviance(mod1)/(n-p))
X <- model.matrix( ~ log(body) + is.human, data = mammals)
XtX.inv <- solve(t(X) %*% X)


T.1 <- beta.2.hat/(s*sqrt(XtX.inv[3,3]))
p.val <- pt(T.1, n - p, lower.tail = F)
p.val
```
The calculated p-value is `r p.val`.

## c)

We now consider all non-human mammals and construct a one-sided prediction interval for the human brain size. For ease of notation, we define $z_i := \ln y_i$ and $v_i = \ln(x_{bi})$. We also let $n' = n -1$ as the number of observations (since we exclude humans). Now,  $z_h = \beta_0 + \beta_1 v_h + \varepsilon_h$ is the stochastic variable from which the log of the human brain mass is realized and $\hat{z}_h = \hat{\beta}_0 + \hat{\beta}_1v_h$ is the corresponding estimate. Then we can find the pivotal quantity

$$
T_2 = \frac{z_h - \widehat{z}_h}{s\sqrt{1 + 1/n' + \frac{(v_h - \bar{v})^2}{\sum_{i = 1}^{n'}(v_i-\bar{v})^2}}} \sim t_{n' -2}.
$$
We refer to the good old [\textcolor{blue}{subject-pages}](https://tma4245.math.ntnu.no) (simple linear regression/prediction and prediction intervals in simple linear regression) for this result. Thus, we can find the one-sided prediction interval:

$$
P(T_2 < k) = 1 - \alpha \implies k = t_{n'-2,\ \alpha}.
$$
Rearranging, we arrive at

$$
P\left(z_h < \underbrace{t_{n'-2,\ \alpha} \cdot s\sqrt{1 + 1/n' + \frac{(v_h - \bar{v})^2}{\sum_{i = 1}^n(v_i-\bar{v})^2}} + \widehat{z}_h}_{=\ \ln U} \right) = 1 - \alpha.
$$
Raising $e$ to the power of both sides of the inequality, we get
$$
P(y_h < U) = 1-\alpha.
$$
In accordance with the task description, we define

$$
A = \{y_h \notin (-\infty,\ U)\} = \{y_h \ge U\}, \quad \text{and} \quad B = \{ T_1 \ge t_{n-p,\ \alpha}\}
$$
We now observe that $A$ is equivalent to $\{T_2 \ge t_{n'-2,\ \alpha}\} = \{T_2 \ge t_{n-p,\ \alpha}\}$, where $p = 3$ as before. To show that $A$ and $B$ are equivalent, we find the MLE of $\beta_2$ from the model in b) by considering the profile log-likelihood (here $\boldsymbol{x}_i$ denotes the $i$'th row of the previously defined design matrix) :
$$
\begin{split}
l_p(\beta_0,\beta_1) &= \sup_{\beta_2} l(\beta_0,\beta_1, \beta_2)\\
&= \sup_{\beta_2}\ln\left( \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}\sigma}  e^{-\frac{1}{2}\left(\frac{z_i-\boldsymbol{x}_i^T\boldsymbol{\beta}}{\sigma}\right)^2}\right) \\
&= \sup_{\beta_2} \left( n\ln\left(\frac{1}{\sqrt{2\pi}\sigma}\right) - \frac{1}{2\sigma^2}\sum_{i = 1}^n (z_i-\boldsymbol{x}_i^T\boldsymbol{\beta})^2\right)\\
&= \sup_{\beta_2} \left( n\ln\left(\frac{1}{\sqrt{2\pi}\sigma}\right) - \frac{1}{2\sigma^2}\sum_{\begin{matrix}i=1\\i\ne h\end{matrix}}^n (z_i-\boldsymbol{x}_i^T\boldsymbol{\beta})^2- \frac{1}{2\sigma^2} (z_h-\boldsymbol{x}_h^T\boldsymbol{\beta})^2\right).
\end{split}
$$
Since $x_{hi}$ is nonzero for only one term in the sum above (for $i = h$) we only need to consider this term. That is, the term with $\boldsymbol{x}_h := \begin{bmatrix}1 & \ln x_{bh} & 1\end{bmatrix}^T$. The constant in front of $(z_h-\boldsymbol{x}_h^T\boldsymbol{\beta})^2$ is negative, so the supremum is attained when this is equal to zero. Thus,

$$
(z_h-\boldsymbol{x}_h^T\boldsymbol{\beta})^2 = 0 \quad \implies \quad z_h - \beta_0 - \beta_1 v_h - \beta_2 = 0,
$$

which means that $\beta_2 = z_h - \beta_0 - \beta_1v_h$. Due to the invariance of MLEs, we now know that
$$
\hat{\beta}_2 = z_h - \hat{\beta}_0 - \hat{\beta_1}v_h = z_h - \widehat{z}_h.
$$
We also note that the estimators $\hat{\beta}_0$ and $\hat{\beta}_1$ are the same here as in the case where we do not consider humans (since the term involving $\boldsymbol{x}_h$ in the log-likelihood evaluates to zero). Thus, since both $T_1$ and $T_2$ depend on the same $\hat{\beta}_2 = z_h - \widehat{z}_h$, meaning that $A$ and $B$ occur when the difference $z_h - \widehat{z}_h$ is large, we can conclude that the two events are equivalent. 

## d)
For a gamma-distributed random variable, the pdf takes the form
$$
f(x; a,b) = \frac{b^a}{\Gamma(a)} x^{a-1}e^{-bx}.
$$
Using the parametrization $\mu = \frac{a}{b}$ and $\nu = a$, we construct the GLM with a log-link as follows.
$$
y_i \sim \mathrm{Gamma}(\mu_i,\ \nu_i),
$$
with $\text{E}[Y_i] = \mu_i$, such that
$$
\ln(\mu_i) = \eta_i = \boldsymbol{x}_i^T \boldsymbol{\beta}.
$$
Next, we fit the model (note that we use the logarithm of the body mass):
```{r}
mod.gamma <- glm(brain ~ log(body) + is.human, family = Gamma(link = "log"), data = mammals)
summary(mod.gamma)
```


## e)
We want to test whether the following relationship holds (recall earlier notation):
$$
y_i = y_0x_{bi}^{3/4},
$$
where $y_i$ is the brain mass, $y_0$ is a constant and $x_{bi}$ is the body mass. Since this is equivalent to testing

$$
\ln(y_i) = \ln(y_0) + \frac{3}{4}\ln(x_{bi}),
$$
this simply amounts to performing the hypothesis test:
$$
H_0: \beta_1 = \frac{3}{4} \quad \mathrm{vs.} \quad \beta_1 \ne \frac{3}{4}.
$$

### Linear model
We first consider the linear model from (b), and construct a Wald test:

```{r}
# Wald test:
C <- matrix(c(0, 1, 0), nrow = 1)
d <-  as.vector(3/4)
r <- 1
p <- 3
n <-  nrow(mammals)
beta.hat <- mod1$coefficients
s2 <- deviance(mod1)/(n-p)
X <- model.matrix(mod1)
XtX.inv <- solve(t(X) %*% X)

w <- t((C %*% beta.hat - d)) %*% solve(s2*C %*% XtX.inv %*% t(C)) %*% (C %*% beta.hat - d)
p.val <- pchisq(w, r, lower.tail = FALSE)
p.val
```

The likelihood-ratio test for the linear model can be carried out as follows:

```{r}
mod1.offset <- lm(log(brain) ~ is.human, offset = 3/4*log(body), data = mammals)
anova(mod1.offset, mod1, test = 'LRT')
```

### Gamma-GLM
For a generalized linear model, the Wald statistic can be written as

$$
w =(C\hat{\boldsymbol{\beta}} - d)^T[CF^{-1}(\hat{\boldsymbol{\beta}})C^T]^{-1}(C\hat{\boldsymbol{\beta}} - d),
$$

which is asymptotically $\chi^2$-distributed with $r = \mathrm{rank}(C)$ degrees of freedom. We compute its value:
```{r}
beta.hat <- as.vector(mod.gamma$coefficients)
w <- t(C %*% beta.hat - d) %*% solve(C %*% vcov(mod.gamma) %*% t(C)) %*% (C %*% beta.hat - d)

p.val <- pchisq(w, r, lower.tail = FALSE)
p.val
```

Next, we perform an LR-test for the GLM:
```{r}
mod.gamma.offset <- glm(brain ~ 1 + is.human, family = Gamma(link = "log"),
                        offset = 3/4*log(body), data = mammals) 

anova(mod.gamma.offset, mod.gamma, test = "LRT")
```

For both the linear model and for the gamma-GLM, there is no evidence that we should reject $H_0$, which supports the hypothesis that the relationship $y_i = y_0x_{bi}^{3/4}$ holds.

We observe that the p-values for the Wald and LR-test are essentially equal for the linear model, while for the GLM, the difference in p-values is notable. The reason behind this is that the LR-test and Wald test are equivalent for the linear model. This can be shown by noting that the Wald-statistic is equal to the F-statistic, since $W = rF = F$ (see Fahrmeir p. 131). The LRT-statistic is, in turn, is a strictly monotonic function of the F-statistic, which shows that the two tests are equivalent. 

For the GLM, on the other hand, this is not the case, and even though the Wald test-statistic, $w$, and the LRT-statistic, $lr$, are asymptotically equivalent, where $w,lr \overset{a}{\sim} \chi^2_r$ (Fahrmeir p. 664), they can give quite different results for finite samples. The likelihood ratio test is generally considered more reliable, which is connected to the fact that it considers the model under both hypotheses, while the Wald test only considers the model under the alternative hypothesis. Other reasons to prefer the LR-test are listed [\textcolor{blue}{here}](https://en.wikipedia.org/wiki/Wald_test#Alternatives_to_the_Wald_test).

# f)
We need to be careful comparing the log-likelihoods and hence the AICs of the models, because for the GLM we consider $Y \sim \mathrm{Gamma}$, while in the linear model we consider $\ln Y\sim \mathrm{Normal}$. To make them comparable, we define $X:= \ln(Y)$. Then (for the linear model) $Y = e^X$ and the Jacobian transformation yields a density of

$$
f_Y(y) = \left|\frac{\partial x}{\partial y}\right| f_X(x) = \frac{1}{y}f_X(x).
$$
This then yields a log-likelihood:

$$
l_Y(\boldsymbol{\beta}) = l_X(\boldsymbol{\beta}) -\sum_{i = 1}^n \ln y_i,
$$
where  $l_X(\boldsymbol{\beta})$ is the log-likelihood of the original linear model. We implement this 'correction' in the calculation of AIC below: 
```{r}
p = 3
AIC.linear <- 2*p + 2*(logLik(mod1) - sum(log(mammals$brain)))
AIC.gamma <- 2*p + 2*logLik(mod.gamma)
AIC.linear
AIC.gamma
```
We see that the gamma-GLM is superior to the linear mode with respect to AIC.

### Theoretical skew of log of gamma distribution:

Let $Y$ be gamma distributed with shape parameter $a$ and rate parameter $b$.
The moment generating function for $\ln Y$ is
$$
    M_{\ln Y}(t) = \text{E}[e^{t \ln Y}] = \text{E}[Y^t],
$$
where the expectation can be calculated as
$$
\begin{split}
    \text{E}[Y^t] &= \int_0^\infty \frac{b^a}{\Gamma(a)} y^{t+a-1} e^{-b y}\ \text{d}y \\
    &= \frac{b^a}{\Gamma(a)} \int_0^\infty y^{t+a-1} e^{-b y}\ \text{d}y \\
    &= \frac{b^a}{\Gamma(a)} \int_0^\infty \left(\frac{\xi}{b}\right)^{t+a-1} e^{-\xi}\ \frac{\text{d}\xi}{b} \\
    &= \frac{b^{-t}}{\Gamma(a)} \int_0^\infty \xi^{t+a-1} e^{-\xi}\ \text{d}\xi \\
    &= \frac{b^{-t}}{\Gamma(a)}\ \Gamma(t+a),
\end{split}
$$
where we used the substitution $\xi = by$. The cumulant-generating function is defined as the log of the moment generating function, $K(t) := \ln M(t)$, so it follows that
$$
    K_{\ln Y}(t) = \ln M_{\ln Y}(t) = - t \ln b + \ln \Gamma(t+a) - \ln \Gamma(a).
$$
The first cumulat is $K_{\ln Y}^{(1)}(0) = \dfrac{\text{d} K_{\ln Y}(t)}{\text{d} t}\Big|_{t=0} = - \ln b + \psi(a)$, where $\psi^{(0)}(x) = \frac{\Gamma'(x)}{\Gamma(x)}$ is the digamma function. The subsequent cumulants can be derived using the polygamma functions. Recall that
the polygamma function of order $m$ is defined as
$$
    \psi^{(m)}(x) = \frac{\text{d}^{m+1} }{\text{d} x^{m+1}} \ln \Gamma (x),
$$
so the subsequent cumulants are $K_{\ln Y}^{(n)}(t) = \psi^{(n-1)}(a)$ for $n\ge 2$.

The skew of a random variable $X$ with mean $\mu$ and variance $\sigma$ is defined as
$$
    \text{Skew}[X] := \text{E}\left[\left(\frac{X-\mu}{\sigma}\right)^3\right],
$$
so it follows that 
$$
    \text{Skew}[\ln Y] = \frac{\text{E}\left[\left(\ln Y -\text{E}[\ln Y]\right)^3\right]}{\left(\text{Var}({\ln Y})\right)^{3/2}},
$$

where the numerator is the third central moment, equal to the third cumulant and the variance is equal to the second cumulant. Thus the skew of the log of the gamma distribution is
$$
    \text{Skew}[\ln Y] = \frac{\psi^{(2)}(a)}{\left(\psi^{(1)}(a)\right)^{3/2}}.
$$
In R, GLM with gamma-distribution assumes the shape parameter $a$ to be constant. To satisfy this condition, a dispersion parameter $\phi := \frac1a$ is introduced, which can be found in the summary. The polygamma functions are calculated using the library `pracma`.

```{r}
psi <- pracma::psi

phi <- summary(mod.gamma)$dispersion
a <- 1/phi

theory.skew <- psi(2,a) / (psi(1,a))^(3/2)
theory.skew
```
This gives the estimate for the skew of the log mammalian brain size  as `r theory.skew`.


### Sample skew of residuals from the LM in (a):

The sample skew is defined as
$$
  \text{Sample skew} := \frac{\frac1n \sum_{i=1}^{n}\left(x_i-\bar x\right)^3}{\left[\frac1{n-1} \sum_{i=1}^{n}\left(x_i-\bar x\right)^2\right]^{3/2}}.
$$
For the linear model fitted in a), we calculate the sample skew of the residuals:
```{r}
# residuals from LM in a)
x <- residuals(mod0)

m.3 <- 1/length(x) * sum((x - mean(x))^3)
s <- sd(x) # = sqrt(1/(length(x)-1) * sum((x-mean(x))^2)) 

sample.skew <- m.3/s^3
sample.skew
```

We observe that the estimated skew of the log-gamma distributed variable is negative and larger in absolute value than the sample skew of the residuals of the linear model from (a). This arguably makes the the gamma-GLM unsuitable, since the log-skew does not match the skew of the residuals. However, as previously observed, the gamma-GLM is superior with respect to the AIC. Side note: Why do we compare the skew of $\ln Y$ to the sample skew of the residuals from the linear model, i.e. $\ln Y_{\text{lin}} - \ln \widehat{Y}_{\text{lin}}$? The reason is that $Y_{\text{lin}} \sim \mathcal{N}(\boldsymbol{x}_i^T \boldsymbol{\beta}, \sigma^2)$, i.e. its mean depends on the covariates. To remedy this, we subtract the predictions, which gives us the residuals: $\ln Y_{\text{lin}} - \ln \widehat{Y}_{\text{lin}} \sim \mathcal{N}(0, \sigma^2)$. Then we are safe to calculate the sample skew.


\newpage
# Problem 2


## Assumptions

In this problem we apply ordinal multinomial regression to data from Norway Chess 2021. The response variable $y_i$ is the outcome of the $i$'th match. This can be considered an ordered categorical variable
$$
  y_i =
  \begin{cases}
    1 &,\quad \text{white win} \\ 2 &, \quad \text{draw}\\ 3 &, \quad \text{black win},
  \end{cases}
$$
which may depend on relative strength of different players, which player plays white and black and the type of game played. The response can be determined by an underlying latent variable $u_i$, given by
$$
  u_i = -\boldsymbol{x}_i^T \boldsymbol{\beta} + \epsilon_i,
$$
where $\epsilon_i \overset{iid}\sim f$, where $f$ is some standard distribution with cdf $F$. In this model, the event $y_i = r$ occurs if $\theta_{r-1} < u_i \le \theta_{r}$ for some parameters $\{\theta_i\}_{i=0}^3$ satisfying
$$
  -\infty = \theta_0 < \theta_1 < \theta_2 < \theta_3 = \infty.
$$
It follows that
$$
  \text P(y_i \le r) = \text P(u_i \le \theta_r) = \text P(\epsilon_i \le \theta_r + \boldsymbol{x}_i^T \boldsymbol{\beta}) = F(\theta_r + \boldsymbol{x}_i^T \boldsymbol{\beta}),
$$
so the probability of observing a particular outcome of the $i$'th match becomes
$$
  \begin{split}
  \pi_{ir} = P(y_i = r) &= P(y_i \le r) - P(y_i \le r-1)\\
  &= F(\theta_r + \boldsymbol{x}_i^T \boldsymbol{\beta}) - F(\theta_{r-1} + \boldsymbol{x}_i^T \boldsymbol{\beta}).
  \end{split}
$$
This means that our model returns that white wins whenever $u_i \le \theta_1$, draw if $\theta_1 < u_i \le \theta_2$ and black win for $u_i > \theta_2$. Since ordinal regression only depend on relative orderings, we do not need to include the intercept $\beta_0$ in $\boldsymbol{\beta}$, as then the values of the thresholds $\theta$s could be shifted by subtracting the value of the intercept. Using the notation $\boldsymbol{\beta} = \begin{bmatrix}\beta_1 & \cdots & \beta_k\end{bmatrix}^T$, the unknown parameters are therefore $\{ \theta_1, \theta_2, \beta_1, \cdots, \beta_k \}$.

## About the data

Our data set consists of $n=44$ chess matches played with six players, where the set of players are
$$
  \text{P} = \{\text{carlsen},\ \text{firouzja},\ \text{karjakin},\ \text{nepomniachtchi},\ \text{rapport},\ \text{tari}\}.
$$
To get a better understanding of the data set, we give a recap of the tournament regulations. The matches was played over 10 rounds of classical games. The event was a double round-robin, meaning that two players played against each other in two separate rounds, one time with the white pieces and the other time with black. If the result of a the classical game was a draw, then the two players faced each other again to play an armageddon game with the same pieces as in the classical game. In each round, the six players played the three classical games simultaneously. In total, this results in 30 classical games. The remaining 14 comes from the armageddon games.

For each round, the reward was 3 points for winning the classical game and 0 for loosing. If the classical game ended in a draw the armageddon game yielded 1.5 points for winning and 1 point for loosing. If this game also ended in a draw, the winner is declared to be the player with the black pieces. This means that the outcome "draw" and "black win" gave the same points for the armageddon format.

The complete data set can be seen on the next page.

\newpage
```{r}
df <- read.csv('data/Norway\ Chess\ 2021.csv')

df$black = as.factor(df$black)
df$white = as.factor(df$white)
df$type = as.factor(df$type)
df$y <- factor(df$y, ordered=TRUE)
df
```

\newpage


# Assumptions about the residuals

### Propositional odds model / Cummulative Logit

$$
  F(x) = \frac{e^x}{1+e^x}, \qquad \epsilon_i \sim \text{Logistic}(0,\ 1)
$$
Using a propositional odds model, the residuals are logistic distributed.


### Cumulative Probit

$$
  F(x) = \Phi(x), \qquad \epsilon_i \sim N(0,\ 1)
$$
Using a cumulative probit model, the residuals are normally distributed.

### Cumulative Gumbel

$$
  F(x) = e^{-e^{x}}, \quad \epsilon_i \sim \text{Gumbel}(x; 0, 1) = e^{-(x+e^{-x})}
$$

### AICc

Since we have a small number of observations, we also consider AICc, which adds a correction term to AIC, given by
$$
  \text{AICc} := \text{AIC} + \frac{2k^2 + 2k}{n-k-1},
$$
where it is usually assumed that the residuals are normally distributed. We will ignore this for now, although this is not the case for the cumulative logit and Gumbel models.



\newpage
# Cumulative logit models without interactions

### Model 0

Lets first assume that there are no interactions. Define a family of models on the form
$$
u_i = -(\alpha_{w(i)} + \beta_{b(i)} + \gamma_{t(i)} ) + \varepsilon_i,
$$
where $\alpha_{w(i)}$ is the effect of player $w(i)$ having white pieces, $\beta_{b(i)}$ is the effect of player $b(i)$ having black pieces and $\gamma_{t(i)}$ is the effect of type $t(i)$ of game played. Since the design matrix must have full rank, R automatically chooses the reference group for the different parameters. This is by default chosen to be the first in the lexicographic order. As so, "carlsen" will be the reference in both $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$, and "armageddon" will be the reference for type of game. As there are more classical games, we change the reference type using the relevel() function.

```{r}
library(VGAM)

# change "classic" to reference type
df <- within(df, type <- relevel(type, ref = "classic"))

fit00<- vglm(y ~ white + black + type,
              family=cumulative(parallel = TRUE, link="logitlink"), data=df)

c(AIC = AIC(fit00), AICc = AICc(fit00))
anova(fit00, type = 3, test = "LRT")
summary(fit00)
```

From the summary, we observe that `type` has a high p-value, so we try to fit a model without a `type` covariate and compare the models with a deviance-based test.
```{r}
fit01 <-  vglm(y ~ white + black,
              family=cumulative(parallel = TRUE, link="logitlink"), data=df)
anova(fit01, fit00, type = 1, test = 'LRT')
```
The test does not yield evidence that the model without `type` is more suitable, but the AIC and AICc are lower:

```{r}
c(AIC = AIC(fit01), AICc = AICc(fit01))
```



```{r, echo = FALSE, eval = F}
# P(u <= theta_1), P(u <= theta_2)
p.less_or_equal <- plogis(predict(fit.0, df))
stats <- cbind('white'=df$white, 'black'=df$black,
               'P(white)'=round(p.less_or_equal[,1],2),
               'P(draw)'=round(p.less_or_equal[,2]-p.less_or_equal[,1],2),
               'P(black)'=round(1-p.less_or_equal[,2],2),
               'outcome'=c('white','draw','black')[df$y])
#stats
```

\newpage
### Model 1

Since it could be argued that the given skill level of a player could be the same for playing both white and black, we next consider the model where the effect of playing white is equal to the effect of playing black, meaning $\alpha_j = - \beta_j, \quad j = 1,2,\ldots, k$. This would be a simpler model since there are fewer parameters to estimate. The model becomes
$$
u_i = -(\alpha_{w(i)} - \alpha_{b(i)} + \gamma_{t(i)} ) + \varepsilon_i.
$$

To make this new model, we will do a dummy-encoding of the players, with value 1 if the player is white and -1 if the player is black. To make the design matrix have full rank, we drop `carlsen`.
```{r}
dummy_cols <- fastDummies::dummy_cols

# dummy-encode white, add -1 for black
df.dummy <- dummy_cols(df, select_columns = "white")
for(i in 1:nrow(df.dummy)){
    df.dummy[i, paste("white", df.dummy$black[i], sep="_")] = -1
}
# remove black, white, white_carlsen
df.dummy <- subset(df.dummy, select = -c(black, white, white_carlsen))
```

```{r}
# alpha = - beta
fit10 <- vglm(y ~ . - round,
              family=cumulative(parallel = TRUE, link="logitlink"), data=df.dummy)
summary(fit10)
c(AIC = AIC(fit10), AICc = AICc(fit10))
```

Again, we see that `type` has a high p-value, and we therefore consider a model without it:
```{r}
fit11 <- vglm(y ~ . - round - type ,
              family=cumulative(parallel = TRUE, link="logitlink"), data=df.dummy)
anova(fit10, fit11, type = 1, test = 'LRT')
```

```{r}
G2 = deviance(fit11) - deviance(fit10)
df.diff = df.residual(fit11) - df.residual(fit10) 
G2
1 - pchisq(G2, df.diff)
```
The test gives no reason to prefer the simpler model, but the values for AIC and AICc are lower:
```{r}
c(AIC = AIC(fit11), AICc = AICc(fit11))
```
```{r}
fit.simple3 <- vglm(y ~ . -round, family=cumulative(parallel = FALSE ~ type, link="logitlink"), data = df.dummy)
summary(fit.simple3)
AIC(fit.simple3)
```


\newpage
### Model 2
\textcolor{red}{I moved this under model 1}

Same as model 1, but here we also assume that there are no effect of type of game played, thus
$$
  u_i = -(\alpha_{w(i)} - \alpha_{b(i)}) + \varepsilon_i.
$$

```{r}
# alpha = - beta, gamma = 0
fit.2 <- vglm(y ~ . - type - round,
              family=cumulative(parallel = TRUE, link="logitlink"), data=df.dummy)
summary(fit.2)
c(AIC = AIC(fit.2), AICc = AICc(fit.2))
```

\newpage
### Model 3

From the models so far, we see that the model with the lowest AIC and AICc is model 2, the model consisting of only the relative strengths of each player. In the summary, we see that the relative strength of some players are close to one another, in particular "firouzja" and "rapport", but also note that "nepomniachtchi" and "karjakin" are comparable. Let us try fitting a model where we assume that the relative strength of "firouzja" and "rapport" are equal. Thus our model becomes
$$
  u_i = -(\alpha_{w(i)} - \alpha_{b(i)}) + \varepsilon_i,
$$
where $\alpha_{\text{firouzja}} = \alpha_{\text{rapport}}$.

```{r}
fit.3 <- vglm(y ~ I(white_firouzja + white_rapport)
              + white_nepomniachtchi + white_karjakin + white_tari,
              family=cumulative(parallel = TRUE, link="logitlink"), data=df.dummy)
summary(fit.3)
c(AIC = AIC(fit.3), AICc = AICc(fit.3))
```

\newpage
### Model 4
Let us try fitting a model where we assume that the relative strength of "nepomniachtchi" and "karjakin" are equal. Thus our model becomes
$$
  u_i = -(\alpha_{w(i)} - \alpha_{b(i)}) + \varepsilon_i,
$$
where $\alpha_{\text{nepomniachtchi}} = \alpha_{\text{karjakin}}$.


```{r}
fit.4 <- vglm(y ~ I(white_firouzja + white_rapport)
              + I(white_nepomniachtchi + white_karjakin) + white_tari,
              family=cumulative(parallel = TRUE, link="logitlink"), data=df.dummy)
summary(fit.4)
c(AIC = AIC(fit.4), AICc = AICc(fit.4))
```

\newpage
### Model 5
Let us try fitting a model where we assume that the relative strength of all players except 'carlsen' are equal. Thus our model becomes
$$
  u_i = -(\alpha_{w(i)} - \alpha_{b(i)}) + \varepsilon_i,
$$
where $\alpha_{\text{carlsen}} = 0$ and $\alpha_{\text{not carlsen}} \ne 0$.

```{r}
fit.5 <- vglm(y ~ I(white_firouzja + white_rapport+ white_nepomniachtchi
                    + white_karjakin + white_tari),
              family=cumulative(parallel = TRUE, link="logitlink"), data=df.dummy)
summary(fit.5)
c(AIC = AIC(fit.5), AICc = AICc(fit.5))
```


```{r}
fit.5 <- vglm(y ~ I(white_firouzja + white_rapport+ white_nepomniachtchi
                    + white_karjakin + white_tari) + type,
              family=cumulative(parallel = FALSE ~type, link="logitlink"), data=df.dummy)
c(AIC = AIC(fit.5), AICc = AICc(fit.5))
```




```{r, eval = FALSE, echo = FALSE}
X = data.frame(matrix(0, nrow(df), nlevels(df$black)))
colnames(X) <- levels(df$black)

for(i in 1:nrow(df)){
  black = as.character(df$black[i])
  white = as.character(df$white[i])
  X[i,black] = 1
  X[i, white] = -1
}
X$y = df$y

fit.simple <- vglm(y ~ ., family=cumulative(parallel = TRUE, link="logitlink"), data=X[2:ncol(X)])
summary(fit.simple)
anova(fit, fit.simple, test = "LRT", type = 1)
AIC(fit.simple)
df.residual(fit)

# Simple with type
X$type = df$type
fit.simple2 <- vglm(y ~ ., family=cumulative(parallel = TRUE, link="logitlink"), data=X[2:ncol(X)])
AIC(fit.simple2)
summary(fit.simple2)

fit.simple3 <- vglm(y ~ . -round, family=cumulative(parallel = FALSE ~ type, link="logitlink"), data = df.dummy)
AIC(fit.simple3)
summary(fit.simple3)
```

