---
title: 'TMA4315: Project 2'
author: "jototlan@stud.ntnu.no (10018), martigtu@stud.ntnu.no (10037)"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1
```{r}
mammals <- read.table(
  "https://www.math.ntnu.no/~jarlet/statmod/mammals.dat",
  header=T)
```

## a)
```{r}
plot(log(mammals$body), log(mammals$brain))
```


The log-log plot of the brain mass against body mass seems to reveal a linear trend. We thus fit the following model:

```{r}
mod0 <- lm(log(brain) ~ log(body), data = mammals)
summary(mod0)
```

If we let $\boldsymbol{y} = [y_1, \ldots, y_n]^T$ denote the brain mass and $\boldsymbol{x}_b = [x_{b1}, \ldots, x_{bn}]^T$ denote the corresponding body mass, we have fitted the model $\ln(y_i) = \beta_0 + \beta_1 \ln(x_{bi}))$, $i = 1,2\ldots n$, with parameter estimates given in the summary above.

## b)
The extended model is fitted below.
```{r}
mammals$is.human = as.factor(mammals$species == "Human")

mod1 <- lm(log(brain) ~ log(body) + is.human, data = mammals)
summary(mod1)
```
Let $\hat{\boldsymbol{\beta}} = \begin{bmatrix}\hat{\beta}_0 & \hat{\beta}_1 & \hat{\beta}_2\end{bmatrix}^T$ be the coefficient estimates given in the summary above, where $\hat{\beta}_2 \approx$ `r mod1$coefficients[3]` models the effect which being a human has on the (log of) brain size. Since we have used a log-transform on both the brain mass and body mass, humans will according to the model be larger by a factor of $e^{\hat{\beta}_2} =$ `r exp(mod1$coefficients[3])`.

We use the notation $\boldsymbol{y} = X\boldsymbol{\beta} + \boldsymbol{\varepsilon}$ to represent the linear model. Here, $X$ is the $n \times p$ design matrix, where $n$ is the number of observations and $p$ is the number of parameters used in the model. Here, $X = \begin{bmatrix}\boldsymbol{1} & \ln\boldsymbol{x}_b & \boldsymbol{x}_h\end{bmatrix}$, where $\boldsymbol{x}_h = [x_{h1}, \ldots, x_{hn}]^T = [0, \ldots 0, 1, 0, \ldots, 0]^T$ which has a nonzero entry $x_{hh} = 1$ only for humans only. As usual, $\boldsymbol{\varepsilon} \sim \mathcal{N}(\boldsymbol{0}, \sigma^2 I_n)$. We know that

$$
\hat{\boldsymbol{\beta}} \sim \mathcal{N}(\boldsymbol{\beta}, \sigma^2(X^TX)^{-1}).
$$
Now we want to perform the hypothesis test

$$
H_0: \beta_2 = 0 \quad \text{vs.} \quad H_1: \beta_2 > 0.
$$
Under $H_0$, we obtain that (we also index from 0 in the design matrix)

$$
\frac{\hat{\beta}_2}{\sigma\sqrt{(X^TX)^{-1}_{2,2}}} \sim \mathcal{N}(0, 1).
$$

Combining this with the fact that 
$$
\frac{(n-p)s^2}{\sigma^2} \sim \chi^2_{n-p},
$$
where $s^2 = SSE/(n-p)$, we obtain the test statistic

$$
T_1 = \frac{\hat{\beta}_2}{s\sqrt{(X^TX)^{-1}_{2,2}}} \sim t_{n-p},
$$
under $H_0$. We perform the calculations in R:
```{r}
n <- nrow(mammals)
p <- 3
beta.2.hat <- mod1$coefficients[3]
s <- sqrt(deviance(mod1)/(n-p))
X <- model.matrix( ~ log(body) + is.human, data = mammals)
XtX.inv <- solve(t(X) %*% X)


T.1 <- beta.2.hat/(s*sqrt(XtX.inv[3,3]))
p.val <- pt(T.1, n - p, lower.tail = F)
p.val
```
The calculated p-value is `r p.val`.

## c)

We now consider all non-human mammals and construct a one-sided prediction interval for the human brain size. For ease of notation, we define $z_i := \ln y_i$ and $v_i = \ln(x_{bi})$. We also let $n' = n -1$ as the number of observations (since we exclude humans). Now,  $z_h = \beta_0 + \beta_1 v_h + \varepsilon_h$ is the stochastic variable from which the log of the human brain mass is realized and $\hat{z}_h = \hat{\beta}_0 + \hat{\beta}_1v_h$ is the corresponding estimate. Then we can find the pivotal quantity

$$
T_2 = \frac{z_h - \widehat{z}_h}{s\sqrt{1 + 1/n' + \frac{(v_h - \bar{v})^2}{\sum_{i = 1}^{n'}(v_i-\bar{v})^2}}} \sim t_{n' -2}.
$$
We refer to the good old [\textcolor{blue}{subject-pages}](https://tma4245.math.ntnu.no) (simple linear regression/prediction and prediction intervals in simple linear regression) for this result. Thus, we can find the one-sided prediction interval:

$$
P(T_2 < k) = 1 - \alpha \implies k = t_{n'-2,\ \alpha}.
$$
Rearranging, we arrive at

$$
P\left(z_h < \underbrace{t_{n'-2,\ \alpha} \cdot s\sqrt{1 + 1/n' + \frac{(v_h - \bar{v})^2}{\sum_{i = 1}^n(v_i-\bar{v})^2}} + \widehat{z}_h}_{=\ \ln U} \right) = 1 - \alpha.
$$
Taking both sides of the inequality to the power of $e$, we get
$$
P(y_h < U) = 1-\alpha.
$$
In accordance with the task description, we define

$$
A = \{y_h \notin (-\infty,\ U)\} = \{y_h \ge U\}, \quad \text{and} \quad B = \{ T_1 \ge t_{n-p,\ \alpha}\}
$$
We now observe that $A$ is equivalent to $\{T_2 \ge t_{n'-2,\ \alpha}\} = \{T_2 \ge t_{n-p,\ \alpha}\}$, where $p = 3$ as before. To show that $A$ and $B$ are equivalent, we find the MLE of $\beta_2$ from the model in b) by considering the profile log-likelihood (here $\boldsymbol{x}_i$ denotes the $i$'th row of the previously defined design matrix) :
$$
\begin{split}
l_p(\beta_0,\beta_1) &= \sup_{\beta_2} l(\beta_0,\beta_1, \beta_2)\\
&= \sup_{\beta_2}\ln\left( \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}\sigma}  e^{-\frac{1}{2}\left(\frac{z_i-\boldsymbol{x}_i^T\boldsymbol{\beta}}{\sigma}\right)^2}\right) \\
&= \sup_{\beta_2} \left( n\ln\left(\frac{1}{\sqrt{2\pi}\sigma}\right) - \frac{1}{2\sigma^2}\sum_{i = 1}^n (z_i-\boldsymbol{x}_i^T\boldsymbol{\beta})^2\right)\\
&= \sup_{\beta_2} \left( n\ln\left(\frac{1}{\sqrt{2\pi}\sigma}\right) - \frac{1}{2\sigma^2}\sum_{\begin{matrix}i=1\\i\ne h\end{matrix}}^n (z_i-\boldsymbol{x}_i^T\boldsymbol{\beta})^2- \frac{1}{2\sigma^2} (z_h-\boldsymbol{x}_h^T\boldsymbol{\beta})^2\right).
\end{split}
$$
Since $x_{hi}$ is nonzero for only one term in the sum above (for $i = h$) we only need to consider this term. That is, the term with $\boldsymbol{x}_h := \begin{bmatrix}1 & \ln x_{bh} & 1\end{bmatrix}^T$. The constant in front of $(z_h-\boldsymbol{x}_h^T\boldsymbol{\beta})^2$ is negative, so the supremum is attained when this is equal to zero. Thus,

$$
(z_h-\boldsymbol{x}_h^T\boldsymbol{\beta})^2 = 0 \quad \implies \quad z_h - \beta_0 - \beta_1 v_h - \beta_2 = 0,
$$

which means that $\beta_2 = z_h - \beta_0 - \beta_1v_h$. Due to the invariance of MLEs, we now know that
$$
\hat{\beta}_2 = z_h - \hat{\beta}_0 - \hat{\beta_1}v_h = z_h - \widehat{z}_h.
$$
We also note that the estimators $\hat{\beta}_0$ and $\hat{\beta}_1$ are the same here as in the case where we do not consider humans (since the term involving $\boldsymbol{x}_h$ in the log-likelihood evaluates to zero). Thus, since both $T_1$ and $T_2$ depend on the same $\hat{\beta}_2 = z_h - \widehat{z}_h$, meaning that $A$ and $B$ occur when the difference $z_h - \widehat{z}_h$ is large, we can conclude that the two events are equivalent. 

\textcolor{red}{More precise than this?}

## d)
For a gamma-distributed random variable, the pdf takes the form
$$
f(x; a,b) = \frac{b^a}{\Gamma(a)} x^{a-1}e^{-bx}.
$$
Using the parametrization $\mu = \frac{a}{b}$ and $\nu = a$, we construct the GLM with a log-link as follows.
$$
y_i \sim \mathrm{Gamma}(\mu_i,\ \nu_i),
$$
with $\text{E}[Y_i] = \mu_i$, such that
$$
\ln(\mu_i) = \eta_i = \boldsymbol{x}_i^T \boldsymbol{\beta}.
$$
Next, we fit the model (note that we use the logarithm of the body mass):
```{r}
mod.gamma <- glm(brain ~ log(body) + is.human, family = Gamma(link = "log"), data = mammals)
summary(mod.gamma)
```


## e)
We want to test whether the following relationship holds (recall earlier notation):
$$
y_i = y_0x_{bi}^{3/4},
$$
where $y_i$ is the brain mass, $y_0$ is a constant and $x_{bi}$ is the body mass. Since this is equivalent to testing

$$
\ln(y_i) = \ln(y_0) + \frac{3}{4}\ln(x_{bi}),
$$
this simply amounts to performing the hypothesis test:
$$
H_0: \beta_1 = \frac{3}{4} \quad \mathrm{vs.} \quad \beta_1 \ne \frac{3}{4}.
$$

### Linear model
We first consider the linear model from (b), and construct a Wald test:

```{r}
# Wald test:
C <- matrix(c(0, 1, 0), nrow = 1)
d <-  as.vector(3/4)
r <- 1
p <- 3
n <-  nrow(mammals)
beta.hat <- mod1$coefficients
s2 <- deviance(mod1)/(n-p)
X <- model.matrix(mod1)
XtX.inv <- solve(t(X) %*% X)

w <- t((C %*% beta.hat - d)) %*% solve(s2*C %*% XtX.inv %*% t(C)) %*% (C %*% beta.hat - d)
p.val <- pchisq(w, r, lower.tail = FALSE)
p.val
```

The likelihood-ratio test for the linear model can be carried out as follows:

```{r}
mod1.offset <- lm(log(brain) ~ is.human, offset = 3/4*log(body), data = mammals)
A <- logLik(mod1.offset)
B <- logLik(mod1)

X.stat <- -2 * (as.numeric(A)-as.numeric(B))
p.val <- pchisq(X.stat, df = r, lower.tail = FALSE)
p.val

# Use this instead? ask about this?
anova(mod1, mod1.offset, test = 'LRT')
```

### Gamma-GLM
For a generalized linear model, the Wald statistic can be written as

$$
w =(C\hat{\boldsymbol{\beta}} - d)^T[CF^{-1}(\hat{\boldsymbol{\beta}})C^T]^{-1}(C\hat{\boldsymbol{\beta}} - d),
$$

which is asymptotically $\chi^2$-distributed with $r = \mathrm{rank}(C)$ degrees of freedom. We compute its value:
```{r}
beta.hat <- as.vector(mod.gamma$coefficients)
w <- t(C %*% beta.hat - d) %*% solve(C %*% vcov(mod.gamma) %*% t(C)) %*% (C %*% beta.hat - d)

p.val <- pchisq(w, r, lower.tail = FALSE)
p.val
```

Next, we perform an LR-test for the GLM:
```{r}
mod.gamma.offset <- glm(brain ~ 1 + is.human, family = Gamma(link = "log"), offset = 3/4*log(body), data = mammals) 

A <- logLik(mod.gamma.offset)
B <- logLik(mod.gamma)

X.stat <- -2 * (as.numeric(A)-as.numeric(B))
p.val <- pchisq(X.stat, df = r, lower.tail = FALSE)
p.val
```

We observe that the p-values for the Wald and LR-test are almost equal for the linear model, while for the GLM, the difference is larger. The reason behind this is that the LR-test and Wald test are equivalent for the linear model. This can be shown by noting that the Wald-statistic is equal to the F-statistic, since $W = rF = F$ (see Fahrmeir p. 131). The LRT-statistic is, in turn, a strictly monotonic function of the F-statistic, showing that the two tests are equivalent. 

For the GLM, on the other hand, this is not the case, and even though the Wald test-statistic, $w$, and the LRT-statistic, $lr$, are asymptotically equivalent, where $w,lr \overset{a}{\sim} \chi^2_r$ (Fahrmeir p. 664), they can give quite different results for finite samples. The likelihood ratio test is generally considered more reliable, which is connected to the fact that it considers the model under both hypotheses, while the Wald test only considers the model under the alternative hypothesis. Other reasons to prefer the LR-test are listed [\textcolor{blue}{here}](https://en.wikipedia.org/wiki/Wald_test#Alternatives_to_the_Wald_test).

# f)
We need to be careful comparing the log-likelihoods and hence the AICs of the models, because for the GLM we consider $Y \sim \mathrm{Gamma}$, while in the linear model we consider $\ln Y\sim \mathrm{Normal}$. To make them comparable, we define $X:= \ln(Y)$. Then (for the linear model) $Y = e^X$ and the Jacobian transformation yields a density of

$$
f_Y(y) = \left|\frac{\partial x}{\partial y}\right| f_X(x) = \frac{1}{y}f_X(x).
$$
This then yields a log-likelihood:

$$
l_Y(\boldsymbol{\beta}) = l_X(\boldsymbol{\beta}) -\sum_{i = 1}^n \ln y_i,
$$
where  $l_X(\boldsymbol{\beta})$ is the log-likelihood of the original linear model. We implement this 'correction' in the calculation of AIC below: 
```{r}
p = 3
AIC.linear <- 2*p + 2*(logLik(mod1) - sum(log(mammals$brain)))
AIC.gamma <- 2*p + 2*logLik(mod.gamma)
AIC.linear
AIC.gamma
```
We see that the gamma-GLM is superior to the linear mode with respect to AIC.
### Theoretical skew of log of gamma distribution:

Let $Y$ be gamma distributed with shape parameter $a$ and rate parameter $b$.
The moment generating function for $\ln Y$ is
$$
    M_{\ln Y}(t) = \text{E}[e^{t \ln Y}] = \text{E}[Y^t],
$$
where the expectation can be calculated as
$$
\begin{split}
    \text{E}[Y^t] &= \int_0^\infty \frac{b^a}{\Gamma(a)} y^{t+a-1} e^{-b y}\ \text{d}y \\
    &= \frac{b^a}{\Gamma(a)} \int_0^\infty y^{t+a-1} e^{-b y}\ \text{d}y \\
    &= \frac{b^a}{\Gamma(a)} \int_0^\infty \left(\frac{\xi}{b}\right)^{t+a-1} e^{-\xi}\ \frac{\text{d}\xi}{b} \\
    &= \frac{b^{-t}}{\Gamma(a)} \int_0^\infty \xi^{t+a-1} e^{-\xi}\ \text{d}\xi \\
    &= \frac{b^{-t}}{\Gamma(a)}\ \Gamma(t+a),
\end{split}
$$
where we used the substitution $\xi = by$. The cumulant-generating function is defined as the log of the moment generating function, $K(t) := \ln M(t)$, so it follows that
$$
    K_{\ln Y}(t) = \ln M_{\ln Y}(t) = - t \ln b + \ln \Gamma(t+a) - \ln \Gamma(a).
$$
The first cumulat is $K_{\ln Y}^{(1)}(0) = \dfrac{\text{d} K_{\ln Y}(t)}{\text{d} t}\Big|_{t=0} = - \ln b + \psi(a)$, where $\psi^{(0)}(x) = \frac{\Gamma'(x)}{\Gamma(x)}$ is the digamma function. The subsequent cumulants can be derived using the polygamma functions. Recall that
the polygamma function of order $m$ is defined as
$$
    \psi^{(m)}(x) = \frac{\text{d}^{m+1} }{\text{d} x^{m+1}} \ln \Gamma (x),
$$
so the subsequent cumulants are $K_{\ln Y}^{(n)}(t) = \psi^{(n-1)}(a)$ for $n\ge 2$.

The skew of a random variable $X$ with mean $\mu$ and variance $\sigma$ is defined as
$$
    \text{Skew}[X] := \text{E}\left[\left(\frac{X-\mu}{\sigma}\right)^3\right],
$$
so it follows that 
$$
    \text{Skew}[\ln Y] = \frac{\text{E}\left[\left(\ln Y -\text{E}[\ln Y]\right)^3\right]}{\left(\text{Var}({\ln Y})\right)^{3/2}},
$$

where the numerator is the third central moment, equal to the third cumulant and the variance is equal to the second cumulant. Thus the skew of the log of the gamma distribution is
$$
    \text{Skew}[\ln Y] = \frac{\psi^{(2)}(a)}{\left(\psi^{(1)}(a)\right)^{3/2}}.
$$
In R, GLM with gamma-distribution assumes the shape parameter $a$ to be constant. To satisfy this condition, a dispersion parameter $\phi := \frac1a$ is introduced, which can be found in the summary. The polygamma functions are calculated using the library `pracma`.

```{r}
library(pracma)

phi <- summary(mod.gamma)$dispersion
a <- 1/phi

theory.skew <- psi(2,a) / (psi(1,a))^(3/2)
theory.skew
```
This gives the estimate for the skew of the log mammalian brain size given the body size as `r theory.skew`.


### Sample skew of residuals from the LM in (a):

The sample skew is defined as
$$
  \text{Sample skew} := \frac{\frac1n \sum_{i=1}^{n}\left(x_i-\bar x\right)^3}{\left[\frac1{n-1} \sum_{i=1}^{n}\left(x_i-\bar x\right)^2\right]^{3/2}}.
$$
For the linear model fitted in a), we calculate the sample skew of the residuals:
```{r}
# residuals from LM in a)
x <- residuals(mod0)

m.3 <- 1/length(x) * sum((x - mean(x))^3)
s <- sd(x) # = sqrt(1/(length(x)-1) * sum((x-mean(x))^2)) 

sample.skew <- m.3/s^3
sample.skew
```

We observe that the estimated skew of the log-gamma distributed variable is negative and larger in absolute value than the sample skew of the residuals of the linear model from (a). This arguably makes the linear model more suitable than gamma-GLM, \textcolor{red}{because...}


\newpage
# Problem 2


## Assumptions

In this problem we apply ordinal multinomial regression to data from Norway Chess 2021. The response variable $y_i$ is the outcome of the $i$'th match. This can be considered an ordered categorical variable
$$
  y_i =
  \begin{cases}
    1 &,\quad \text{white win} \\ 2 &, \quad \text{draw}\\ 3 &, \quad \text{black win},
  \end{cases}
$$
which may depend on relative strength of different players, which player plays white and black and the type of game played. The response can be determined by an underlying latent variable $u_i$, given by
$$
  u_i = -\boldsymbol{x}_i^T \boldsymbol{\beta} + \epsilon_i,
$$
where $\epsilon_i \overset{iid}\sim f$, where $f$ is some standard distribution with cdf $F$. In this model, the event $y_i = r$ occurs if $\theta_{r-1} < u_i \le \theta_{r}$ for some parameters $\{\theta_i\}_{i=0}^3$ satisfying
$$
  -\infty = \theta_0 < \theta_1 < \theta_2 < \theta_3 = \infty.
$$
It follows that
$$
  \text P(y_i \le r) = \text P(u_i \le \theta_r) = \text P(\epsilon_i \le \theta_r + \boldsymbol{x}_i^T \boldsymbol{\beta}) = F(\theta_r + \boldsymbol{x}_i^T \boldsymbol{\beta}),
$$
so the probability of observing a particular outcome of the $i$'th match becomes
$$
  \begin{split}
  \pi_{ir} = P(y_i = r) &= P(y_i \le r) - P(y_i \le r-1)\\
  &= F(\theta_r + \boldsymbol{x}_i^T \boldsymbol{\beta}) - F(\theta_{r-1} + \boldsymbol{x}_i^T \boldsymbol{\beta}).
  \end{split}
$$
This means that our model returns that white wins whenever $u_i \le \theta_1$, draw if $\theta_1 < u_i \le \theta_2$ and black win for $u_i > \theta_2$. 

## Models

### Propositional odds model / Cummulative Logit

$$
  F(x) = \frac{e^x}{1+e^x}, \qquad \epsilon_i \sim \text{Logistic}(0,\ 1)
$$
### Cummulative Probit

$$
  F(x) = \Phi(x), \qquad \epsilon_i \sim N(0,\ 1)
$$
First we consider the model where 
$$
u_i = -(\alpha_{j(i)} + \beta_{l(i)} ) + \varepsilon_i,
$$
where $\alpha_{j(i)}$ is the effect of player $j(i)$ having white pieces, and $\beta_{l(i)}$ is the effect of player $l(i)$ having black pieces.

```{r}
library(VGAM)

df <- read.csv('data/Norway\ Chess\ 2021.csv')
head(df)


fit <- vglm(y ~ factor(white) + factor(black),
            family=cumulative(parallel = TRUE, link="logitlink"), data=df)
summary(fit)
AIC(fit)

# P(u <= theta_1), P(u <= theta_2)
p.less_or_equal <- plogis(predict(fit, df))


stats <- cbind('white'=df$white, 'black'=df$black,
               'P(white)'=round(p.less_or_equal[,1],2),
               'P(draw)'=round(p.less_or_equal[,2]-p.less_or_equal[,1],2),
               'P(black)'=round(1-p.less_or_equal[,2],2),
               'outcome'=c('white','draw','black')[df$y])
stats
```

Since it could be argued that a given players skills with one color should be proportional or equal to the skills with another color, we next consider the model where $\alpha_j = \beta_j, \quad j = 1,2,\ldots, k$. The model becomes

$$
u_i = -(\alpha_{j(i)} - \alpha_{l(i)} ) + \varepsilon_i.
$$

To get a design matrix of full rank, we need to remove one of the columns. Here we drop `carlsen`
```{r}
# The 'simpler' model from the lecture (effect of player being white is equal when being black)
df$black = as.factor(df$black)
df$white = as.factor(df$white)
X = data.frame(matrix(0, nrow(df), nlevels(df$black)))
colnames(X) <- levels(df$black)
for(i in 1:nrow(df)){
  black = as.character(df$black[i])
  white = as.character(df$white[i])
  X[i,black] = 1
  X[i, white] = -1
}
X$y = df$y

fit.simple <- vglm(y ~ ., family=cumulative(parallel = TRUE, link="logitlink"), data=X[2:ncol(X)])
summary(fit.simple)
anova(fit, fit.simple, test = "LRT", type = 1)
AIC(fit.simple)
df.residual(fit)

# Simple with type
X$type = df$type
fit.simple2 <- vglm(y ~ ., family=cumulative(parallel = TRUE, link="logitlink"), data=X[2:ncol(X)])
AIC(fit.simple2)
summary(fit.simple2)

fit.simple3 <- vglm(y ~ ., family=cumulative(parallel = FALSE ~ type, link="logitlink"), data=X[2:ncol(X)])
AIC(fit.simple3)
summary(fit.simple3)
```

