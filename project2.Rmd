---
title: 'TMA4315: Project 2'
author: "jototlan@stud.ntnu.no (10018), martigtu@stud.ntnu.no (10037)"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1
```{r}
mammals <- read.table(
  "https://www.math.ntnu.no/~jarlet/statmod/mammals.dat",
  header=T)
```

## a)
```{r}
plot(log(mammals$body), log(mammals$brain))
```


The log-log plot of the brain mass against body mass seems to reveal a linear trend. We thus fit the following model:

```{r}
mod0 <- lm(log(brain) ~ log(body), data = mammals)
summary(mod0)
```

This shows that assuming $\ln(brain) = \beta_0 + \beta_1 \ln(body)$ suggest $brain \propto body^{\beta_1}$ with $\beta_1 \approx$ `r mod0$coefficients[2]`.

## b)

```{r}
mammals$is.human = as.factor(mammals$species == "Human")

mod1 <- lm(log(brain) ~ log(body) + is.human, data = mammals)
summary(mod1)
```
Let $\hat{\boldsymbol{\beta}} = \begin{bmatrix}\hat{\beta}_0 & \hat{\beta}_1 & \hat{\beta}_2\end{bmatrix}^T$ be the coefficient estimates given in the summary above. Then the estimated effect on brain mass from being a human is $\hat{\beta}_2 \approx$ `r mod1$coefficients[3]`. Since we have used a log-transform on both the brain mass and body mass, humans will according to the model be larger by a factor of $e^{\hat{\beta}_2} =$ `r exp(mod1$coefficients[3])`.

We use the notation $\boldsymbol{y} = X\boldsymbol{\beta} + \boldsymbol{\varepsilon}$ to represent the linear model. Here, $X$ is the $n \times p$ design matrix, where $n$ is the number of observations and $p$ is the number of parameters used in the model. As usual, $\boldsymbol{\varepsilon} \sim \mathcal{N}(\boldsymbol{0}, \sigma^2 I_n)$. This (along with the other usual assumptions \textcolor{red}{how much detail is required here??}) gives the well known result:

$$
\hat{\boldsymbol{\beta}} \sim \mathcal{N}(\boldsymbol{\beta}, \sigma^2(X^TX)^{-1}).
$$
Now we want to perform the hypothesis test

$$
H_0: \beta_2 = 0 \quad \text{vs.} \quad H_1: \beta_2 > 0.
$$
Under $H_0$, we obtain that (we also index from 0 in the design matrix)

$$
\frac{\hat{\beta}_2}{\sigma\sqrt{(X^TX)^{-1}_{2,2}}} \sim \mathcal{N}(0, 1).
$$

Combining this with the fact that 
$$
\frac{(n-p)s^2}{\sigma^2} \sim \chi^2_{n-p},
$$
where $s^2 = RSS/(n-p)$, we obtain the test statistic

$$
T_1 = \frac{\hat{\beta}_2}{s\sqrt{(X^TX)^{-1}_{2,2}}} \sim t_{n-p},
$$
under $H_0$. We perform the calculations in R:
```{r}
n <- nrow(mammals)
p <- 3
beta.2.hat <- mod1$coefficients[3]
s <- sqrt(deviance(mod1)/(n-p))
X <- model.matrix( ~ log(body) + is.human, data = mammals)
XtX.inv <- solve(t(X) %*% X)


T.1 <- beta.2.hat/(s*sqrt(XtX.inv[3,3]))
p.val <- pt(T.1, n - p, lower.tail = F)
p.val
```
The calculated p-value is `r p.val`.

## c)

We now consider all non-human mammals and construct a one-sided prediction interval for the (log of) human brain size. Define $n' = n -1$ as the number of observations and let $Y_h = \beta_0 + \beta_1 x_h + \varepsilon_h$ be the stochastic variable from which the log of the human brain mass is realized and $\widehat{Y}_h = \hat{\beta}_0 + \hat{\beta}_1x_h$ be the corresponding estimator. Then we can find the pivotal quantity

$$
T_2 = \frac{Y_h - \widehat{Y}_h}{s\sqrt{1 + 1/n' + \frac{(x_h - \bar{x})^2}{\sum_{i = 1}^{n'}(x_i-\bar{x})^2}}} \sim t_{n' -2}.
$$
We refer to the good old [\textcolor{blue}{subject-pages}](https://tma4245.math.ntnu.no) (simple linear regression/prediction and prediction intervals in simple linear regression) for this result. Thus, we can find the one-sided prediction interval:

$$
P(T_2 < k) = 1 - \alpha \implies k = t_{n'-2,\ \alpha}.
$$
Rearranging, we arrive at

$$
P\left(Y_h < \underbrace{t_{n'-2,\ \alpha} \cdot s\sqrt{1 + 1/n' + \frac{(x_h - \bar{x})^2}{\sum_{i = 1}^n(x_i-\bar{x})^2}} + \widehat{Y}_h}_{=\ U} \right) = 1 - \alpha
$$
We denote the right hand side of the inequality above by $U$ and in accordance with the task description define

$$
A = \{Y_h \notin (-\infty,\ U)\} = \{Y_h \ge U\}, \quad \text{and} \quad B = \{ T_1 \ge t_{n-p,\ \alpha}\}
$$
We now observe that $A$ is equivalent to $\{T_2 \ge t_{n'-2,\ \alpha}\} = \{T_2 \ge t_{n-p,\ \alpha}\}$, where $p = 3$ as before. To show that $A$ and $B$ are equivalent, we find the MLE of $\beta_2$ from the model in b) by considering the profile log-likelihood:
$$
\begin{split}
l_p(\beta_0,\beta_1) &= \sup_{\beta_2} l(\beta_0,\beta_1, \beta_2)\\
&= \sup_{\beta_2}\ln\left( \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}\sigma}  e^{-\frac{1}{2}\left(\frac{y_i-\boldsymbol{x}_i^T\boldsymbol{\beta}}{\sigma}\right)^2}\right) \\
&= \sup_{\beta_2} \left( n\ln\left(\frac{1}{\sqrt{2\pi}\sigma}\right) - \frac{1}{2\sigma^2}\sum_{i = 1}^n (y_i-\boldsymbol{x}_i^T\boldsymbol{\beta})^2\right)\\
&= \sup_{\beta_2} \left( n\ln\left(\frac{1}{\sqrt{2\pi}\sigma}\right) - \frac{1}{2\sigma^2}\sum_{\begin{matrix}i=1\\i\ne h\end{matrix}}^n (y_i-\boldsymbol{x}_i^T\boldsymbol{\beta})^2- \frac{1}{2\sigma^2} (y_h-\boldsymbol{x}_h^T\boldsymbol{\beta})^2\right).
\end{split}
$$
Since $x_{i,2}$ is nonzero for only one term in the sum above, say for  $i = h$, we only need to consider this term. That is, the term with $\boldsymbol{x}_h := \begin{bmatrix}1 & x_h & 1\end{bmatrix}^T$. The constant in front of $(y_h-\boldsymbol{x}_h^T\boldsymbol{\beta})^2$ is negative, so the supremum is attained when this is equal to zero. Thus,

$$
(y_h-\boldsymbol{x}_h^T\boldsymbol{\beta})^2 = 0 \quad \implies \quad y_h - \beta_0 - \beta_1 x_h - \beta_2 = 0,
$$

which means that $\beta_2 = y_h - \beta_0 - \beta_1x_h$. Due to the invariance of MLEs, we now know that
$$
\hat{\beta}_2 = Y_h - \hat{\beta}_0 - \hat{\beta_1}x_h = Y_h - \widehat{Y}_h.
$$
We also note that the estimators $\hat{\beta}_0$ and $\hat{\beta}_1$ are the same here as in the case where we do not consider humans (since the term involving $\boldsymbol{x}_h$ in the log-likelihood evaluates to zero). Thus, since both $T_1, T_2 \propto \hat{\beta}_2$, i.e. both $A$ and $B$ occur when the difference $Y_h - \widehat{Y}_h$ is large, we can conclude that they are equivalent. 

\textcolor{red}{More precise than this?}

## d)
For a gamma-distributed random variable, the pdf takes the form
$$
f(x; a,b) = \frac{b^a}{\Gamma(a)} x^{a-1}e^{-bx}.
$$
Using the parametrization $\mu = \frac{a}{b}$ and $\nu = a$, we construct the GLM with a log-link as follows. Let the mammalian brain size given body size be given as
$$
Y_i \sim \mathrm{Gamma}(\mu_i,\ \nu_i),
$$
with $\text{E}[Y_i] = \mu_i$, such that
$$
\ln(\mu_i) = \eta_i = \boldsymbol{x}_i^T \boldsymbol{\beta}.
$$
Next, we fit the model (note that we use the logarithm of the body mass):
```{r}
mod.gamma <- glm(brain ~ log(body) + is.human, family = Gamma(link = "log"), data = mammals)
summary(mod.gamma)
```


## e)
We want to test whether the following relationship holds:
$$
Y = Y_0M^{3/4},
$$
where $Y$ is the brain mass, $Y_0$ is a constant and $M$ is the brain mass. Since this is equivalent to testing

$$
\ln(Y) = \ln(Y_0) + \frac{3}{4}\ln(M),
$$
This simply amounts to performing the hypothesis test:
$$
H_0: \beta_1 = \frac{3}{4} \quad \mathrm{vs.} \quad \beta_1 \ne \frac{3}{4},
$$
both for the linear model and the GLM. We first consider the linear model, and construct a Wald test:

```{r}
# Wald test:
C <- matrix(c(0, 1, 0), nrow = 1)
d <-  as.vector(3/4)
r <- 1
p <- 3
n <-  nrow(mammals)
beta.hat <- mod1$coefficients
s2 <- deviance(mod1)/(n-p)
X <- model.matrix(mod1)
XtX.inv <- solve(t(X) %*% X)

w <- t((C %*% beta.hat - d)) %*% solve(s2*C %*% XtX.inv %*% t(C)) %*% (C %*% beta.hat - d)
p.val <- pchisq(w, r, lower.tail = FALSE)
p.val
```

The likelihood-ratio test for the linear model can be carried out as follows:

```{r}
mod1.offset <- lm(log(brain) ~ is.human, offset = 3/4*log(body), data = mammals)
A <- logLik(mod1.offset)
B <- logLik(mod1)

X.stat <- -2 * (as.numeric(A)-as.numeric(B))
p.val <- pchisq(X.stat, df = r, lower.tail = FALSE)
p.val
```

For a generalized linear model, the Wald statistic can be written as

$$
w =(C\hat{\boldsymbol{\beta}} - d)^T[CF^{-1}(\hat{\boldsymbol{\beta}})C^T]^{-1}(C\hat{\boldsymbol{\beta}} - d),
$$

which is asymptotically $\chi^2$-distributed with $r = \mathrm{rank}(C)$ degrees of freedom. We compute its value:
```{r}
beta.hat <- as.vector(mod.gamma$coefficients)
w <- t(C %*% beta.hat - d) %*% solve(C %*% vcov(mod.gamma) %*% t(C)) %*% (C %*% beta.hat - d)

p.val <- pchisq(w, r, lower.tail = FALSE)
p.val
```

Next, we perform an LR-test for the GLM:
```{r}
mod.gamma.offset <- glm(brain ~ 1 + is.human, family = Gamma(link = "log"), offset = 3/4*log(body), data = mammals) 

A <- logLik(mod.gamma.offset)
B <- logLik(mod.gamma)

X.stat <- -2 * (as.numeric(A)-as.numeric(B))
p.val <- pchisq(X.stat, df = r, lower.tail = FALSE)
p.val
```

We observe that the p-values for the Wald and LR-test are almost equal for the linear model, while for the GLM, the difference is larger. The reason behind this is that the LR-test and Wald test are equivalent for the linear model. This can be shown by noting that the Wald-statistic is equal to the F-statistic, since $W = rF = F$ (see Fahrmeir p. 131). The likelihood ratio is, in turn, a strictly monotonic function of the F-statistic, showing that the two tests are equivalent. \textcolor{red}{need to show this??}. For the GLM, on the other hand, this is not the case, and the Wald test becomes an approximation to the LR-test (because it only considers the model under the alternative hypothesis, whereas the LR-test consider the model under both hypotheses).

# f)
We need to be careful comparing the log-likelihoods and hence the AICs of the models, because for the GLM we consider $Y \sim \mathrm{Gamma}$, while in the linear model we consider $\ln Y\sim \mathrm{Normal}$. To make them comparable, we define $X:= \ln(Y)$. Then (for the linear model) $Y = e^X$ and the Jacobian transformation yields a density of

$$
f_Y(y) = \left|\frac{\partial x}{\partial y}\right| f_X(x) = \frac{1}{y}f_X(x).
$$
This then yields a log-likelihood:

$$
l_Y(\boldsymbol{\beta}) = l_X(\boldsymbol{\beta}) -\sum_{i = 1}^n \ln y_i,
$$
where  $l_X(\boldsymbol{\beta})$ is the log-likelihood of the original linear model. We implement this 'correction' in the calculation of AIC below: 
```{r}
p = 3
AIC.linear <- 2*p + 2*logLik(mod1) - sum(log(mammals$brain))
AIC.gamma <- 2*p + 2*logLik(mod.gamma)
AIC.linear
AIC.gamma
```

### Teoretical skew of log of gamma distribution:

Let $Y$ be gamma distributed with shape parameter $a$ and rate paramter $b$.
The moment generating function for $\ln Y$ is
$$
    M_{\ln Y}(t) = \text{E}[e^{t \ln Y}] = \text{E}[Y^t],
$$
where the expectation can be calculated as
$$
\begin{split}
    \text{E}[Y^t] &= \int_0^\infty \frac{b^a}{\Gamma(a)} y^{t+a-1} e^{-b y}\ \text{d}y \\
    &= \frac{b^a}{\Gamma(a)} \int_0^\infty y^{t+a-1} e^{-b y}\ \text{d}y \\
    &= \frac{b^a}{\Gamma(a)} \int_0^\infty \left(\frac{\xi}{b}\right)^{t+a-1} e^{-\xi}\ \frac{\text{d}\xi}{b} \\
    &= \frac{b^{-t}}{\Gamma(a)} \int_0^\infty \xi^{t+a-1} e^{-\xi}\ \text{d}\xi \\
    &= \frac{b^{-t}}{\Gamma(a)}\ \Gamma(t+a),
\end{split}
$$
where we used the substitution $\xi = by$. The cumulant-generating function is defined as the log of the moment generating function, $K(t) := \ln M(t)$, so it follows that
$$
    K_{\ln Y}(t) = \ln M_{\ln Y}(t) = - t \ln b + \ln \Gamma(t+a) - \ln \Gamma(a).
$$
The first cumulat is $K_{\ln Y}^{(1)}(0) = \dfrac{\text{d} K_{\ln Y}(t)}{\text{d} t}\Big|_{t=0} = - \ln b + \psi(a)$, where $\psi^{(0)}(x) = \frac{\Gamma'(x)}{\Gamma(x)}$ is the digamma function. The subsequent cumulants can be derived using the polygamma functions. Recall that
the polygamma function of order $m$ is defined as
$$
    \psi^{(m)}(x) = \frac{\text{d}^{m+1} }{\text{d} x^{m+1}} \ln \Gamma (x),
$$
so the subsequent cumulants are $K_{\ln Y}^{(n)}(t) = \psi^{(n-1)}(a)$ for $n\ge 2$.

The skew of a random variable $X$ with mean $\mu$ and variance $\sigma$ is defined as
$$
    \text{Skew}[X] := \text{E}\left[\left(\frac{X-\mu}{\sigma}\right)^3\right],
$$
so it follows that 
$$
    \text{Skew}[\ln Y] = \frac{\text{E}\left[\left(\ln Y -\text{E}[\ln Y]\right)^3\right]}{\left(\text{Var}({\ln Y})\right)^{3/2}},
$$

where the numerator is the third central moment, equal to the third cumulant and the variance is equal to the second cumulant. Thus the skew of the log of the gamma distribution is
$$
    \text{Skew}[\ln Y] = \frac{\psi^{(2)}(a)}{\left(\psi^{(1)}(a)\right)^{3/2}}.
$$
In R, GLM with gamma-distribution assumes the shape parameter $a$ to be constant. To satisfy this condition, a dispersion parameter $\phi := \frac1a$ is introduced, which can be found in the summary. The polygamma functions are calculated using the pracma-library.

```{r}
library(pracma)

phi <- summary(mod.gamma)$dispersion
a <- 1/phi

theory.skew <- psi(2,a) / (psi(1,a))^(3/2)
theory.skew
```
This gives the estimate for the skew of the log mammalian brain size given the body size as `r theory.skew`.


### Sample skew of residuals from the LM in a:

The sample skew is defined as
$$
  \text{Sample skew} := \frac{\frac1n \sum_{i=1}^{n}\left(x_i-\bar x\right)^3}{\left[\frac1{n-1} \sum_{i=1}^{n}\left(x_i-\bar x\right)^2\right]^{3/2}}.
$$
For the linear model fitted in a), we calculate the sample skew of the residuals:
```{r}
# residuals from LM in a)
x <- residuals(mod1)

m.3 <- 1/length(x)*sum((x - mean(x))^3)
s <- sd(x) # = sqrt(1/(length(x)-1) * sum((x-mean(x))^2)) 

sample.skew <- m.3/s^3
sample.skew
```

\textcolor{red}{How does the skew of the log mammalian brain size compare to the sample skew of the residuals of the LM? - The first is negativly skewed, while the residuals are positivly skewed, soo ...}


\newpage
# Problem 2


## Assumptions

In this problem we apply ordinal multinomial regression to data from Norway Chess 2021. The response variable $y_i$ is the outcome of the $i$'th match. This can be considered an ordered categorical variable
$$
  y_i =
  \begin{cases}
    1 &,\quad \text{white win} \\ 2 &, \quad \text{draw}\\ 3 &, \quad \text{black win},
  \end{cases}
$$
which may depend on relative strength of different players, which player plays white and black and the type of game played. The response can be determined by an underlying latent variable $u_i$, given by
$$
  u_i = -\boldsymbol{x}_i^T \boldsymbol{\beta} + \epsilon_i,
$$
where $\epsilon_i \overset{iid}\sim f$, where $f$ is some standard distribution with cdf $F$. In this model, the event $y_i = r$ occurs if $\theta_{r-1} < u_i \le \theta_{r}$ for some parameters $\{\theta_i\}_{i=0}^3$ satisfying
$$
  -\infty = \theta_0 < \theta_1 < \theta_2 < \theta_3 = \infty.
$$
It follows that
$$
  \text P(y_i \le r) = \text P(u_i \le \theta_r) = \text P(\epsilon_i \le \theta_r + \boldsymbol{x}_i^T \boldsymbol{\beta}) = F(\theta_r + \boldsymbol{x}_i^T \boldsymbol{\beta}),
$$
so the probability of observing a particular outcome of the $i$'th match becomes
$$
  \begin{split}
  \pi_{ir} = P(y_i = r) &= P(y_i \le r) - P(y_i \le r-1)\\
  &= F(\theta_r + \boldsymbol{x}_i^T \boldsymbol{\beta}) - F(\theta_{r-1} + \boldsymbol{x}_i^T \boldsymbol{\beta}).
  \end{split}
$$
This means that our model returns that white wins whenever $u_i \le \theta_1$, draw if $\theta_1 < u_i \le \theta_2$ and black win for $u_i > \theta_2$. 

## Models

### Propositional odds model / Cummulative Logit

$$
  F(x) = \frac{e^x}{1+e^x}, \qquad \epsilon_i \sim \text{Logistic}(0,\ 1)
$$
### Cummulative Probit

$$
  F(x) = \Phi(x), \qquad \epsilon_i \sim N(0,\ 1)
$$
First we consider the model where 
$$
u_i = -(\alpha_{j(i)} + \beta_{l(i)} ) + \varepsilon_i,
$$
where $\alpha_{j(i)}$ is the effect of player $j(i)$ having white pieces, and $\beta_{l(i)}$ is the effect of player $l(i)$ having black pieces.

```{r}
df <- read.csv('data/Norway\ Chess\ 2021.csv')

library(VGAM)
head(df)


fit <- vglm(y ~ factor(white) + factor(black),
            family=cumulative(parallel = TRUE, link="logitlink"), data=df)
AIC(fit)

# P(u <= theta_1), P(u <= theta_2)
p.less_or_equal <- plogis(predict(fit, df))


stats <- cbind('white'=df$white, 'black'=df$black,
               'P(white)'=round(p.less_or_equal[,1],2),
               'P(draw)'=round(p.less_or_equal[,2]-p.less_or_equal[,1],2),
               'P(black)'=round(1-p.less_or_equal[,2],2),
               'outcome'=c('white','draw','black')[df$y])
stats
```

Since it could be argued that a given players skills with one color should be proportional or equal to the skills with another color, we next consider the model where $\alpha_j = \beta_j, \quad j = 1,2,\ldots, k$. The model becomes

$$
u_i = -(\alpha_{j(i)} - \alpha_{l(i)} ) + \varepsilon_i.
$$
\textcolor{red}{Need to drop one column from the design matrix in order to get full rank. Why? Silus says you can imagine that one effect dissapears into the intercept...}
```{r}
library(Matrix)
# The 'simpler' model from the lecture (effect of player being white is equal when being black)
df$black = as.factor(df$black)
df$white = as.factor(df$white)
X = data.frame(matrix(0, nrow(df), nlevels(df$black)))
colnames(X) <- levels(df$black)
for(i in 1:nrow(df)){
  black = as.character(df$black[i])
  white = as.character(df$white[i])
  X[i,black] = 1
  X[i, white] = -1
}
rankMatrix((X))
ncol(X)
# X does not have full rank.

X$type = df$type
X$type = as.factor(X$type)
X$y = df$y

fit.simple <- vglm(y ~ ., family=cumulative(parallel = TRUE, link="logitlink"), data=X[2:ncol(X)])
summary(fit.simple)
AIC(fit.simple)

```

