---
title: 'TMA4315: Project 2'
author: "jototlan@stud.ntnu.no (10018), martigtu@stud.ntnu.no (10037)"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1
```{r}
mammals <- read.table(
  "https://www.math.ntnu.no/~jarlet/statmod/mammals.dat",
  header=T)
```

## a)
```{r}
plot(log(mammals$body), log(mammals$brain)) # Seems pretty linear.
```


A log-log plot of the brain mass against body mass seems to reveal a linear trend. We thus fit the following model:

```{r}
mod0 <- lm(log(brain) ~ log(body), data = mammals)
summary(mod0)
```

## b)

```{r}
is.human = ifelse(mammals$species == "Human", 1, 0)
mammals$is.human = as.factor(is.human)

mod1 <- lm(log(brain) ~ log(body) + is.human, data = mammals)
```
Let $\hat{\boldsymbol{\beta}} = [\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2]^T$ be the coefficient estimates given in the summary above. Then the estimated effect on brain mass from being a human is $\hat{\beta}_2 \approx$ `r mod1$coefficients[3]`. Since we have used a log-transform on both the brain mass and body mass, humans will according to the model be larger by a factor of $e^{\hat{\beta}_2} =$ `r exp(mod1$coefficients[3])`.

We use the notation $\boldsymbol{y} = X\boldsymbol{\beta} + \boldsymbol{\varepsilon}$ to represent the linear model. Here, $X$ is the $n \times p$ design matrix, where $n$ is the number of observations and $p$ is the number of parameters used in the model. As usual, $\boldsymbol{\varepsilon} \sim \mathcal{N}(\boldsymbol{0}, \sigma^2 I_n)$. This (along with the other usual assumptions \textcolor{red}{how much detail is required here??}) gives the well known result:

$$
\hat{\boldsymbol{\beta}} \sim \mathcal{N}(\boldsymbol{\beta}, \sigma^2(X^TX)^{-1}).
$$
Now we want to perform the hypothesis test

$$
H_0: \beta_2 = 0 \quad \text{vs.} \quad H_1: \beta_2 > 0.
$$
Under $H_0$, we obtain that (we also index from 0 in the design matrix)

$$
\frac{\hat{\beta}_2}{\sigma\sqrt{(X^TX)^{-1}_{2,2}}} \sim \mathcal{N}(0, 1).
$$

Combining this with the fact that 
$$
\frac{(n-p)s^2}{\sigma^2} \sim \chi^2_{n-p},
$$
where $s^2 = RSS/(n-p)$, we obtain the test statistic

$$
T_1 = \frac{\hat{\beta}_2}{s\sqrt{(X^TX)^{-1}_{2,2}}} \sim t_{n-p},
$$
under $H_0$. We perform the calculations in R:
```{r}
n <- nrow(mammals)
p <- 3
beta.2 <- mod1$coefficients[3]
s <- sqrt(deviance(mod1)/(n-p))
X <- model.matrix( ~ log(body) + is.human, data = mammals)
XtX.inv <- solve(t(X) %*% X)


T.stat <- beta.2/(s*sqrt(XtX.inv[3,3]))
p.val <- pt(T.stat, n - p, lower.tail = F)
p.val
```
The calculated p-value is `r p.val`.

## c)

We now consider all non-human mammals and construct a one-sided prediction interval for the (log of) human brain size. Define $n' = n -1$ as the number of observations and let $Y_h = \beta_0 + \beta_1 x_h + \varepsilon_h$ be the stochastic variable from which the log of the human brain mass is realized and $\widehat{Y}_h = \hat{\beta}_0 + \hat{\beta}_1x_h$ be the corresponding estimator. Then we can find the pivotal quantity

$$
T_2 = \frac{Y_h - \widehat{Y}_h}{s\sqrt{1 + 1/n' + \frac{(x_h - \bar{x})^2}{\sum_{i = 1}^{n'}(x_i-\bar{x})^2}}} \sim t_{n' -2}.
$$
We refer to the good old [\textcolor{blue}{subject-pages}](https://tma4245.math.ntnu.no) (simple linear regression/prediction and prediction intervals in simple linear regression) for this result. Thus, we can find the one-sided prediction interval:

$$
P(T_2 \le k) = 1 - \alpha \implies k = t_{n'-2, \alpha}.
$$
Rearranging, we arrive at

$$
P\left(Y_h \le t_{n-2, \alpha} \cdot s\sqrt{1 + 1/n + \frac{(x_h - \bar{x})^2}{\sum_{i = 1}^n(x_i-\bar{x})^2}} + \widehat{Y}_h \right) = 1 - \alpha
$$
We denote the right hand side of the inequality above by $U$ and in accordance with the task description define

$$
A = \{Y_h > U\}, \quad \text{and} \quad B = \{ T_1 > t_{n-p, \alpha}\}
$$
We now observe that $A$ is equivalent to $\{T_2 > t_{n'-2, \alpha}\} = \{T_2 > t_{n-p, \alpha}\}$, where $p = 3$ as before. To show that $A$ and $B$ are equivalent, we find the MLSE of $\beta_2$ from the model in b) by considering the profile log-likelihood:

$$
\begin{split}
l_p(\beta_1,\beta_2) &= \sup_{\beta_2}\ln\left[ \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}\sigma}  e^{-\frac{1}{2}\left(\frac{y_i-\boldsymbol{x}_i^T\boldsymbol{\beta}}{\sigma}\right)^2}\right] \\
&= \sup_{\beta_2} \left[ n\ln\left(\frac{1}{\sqrt{2\pi}\sigma}\right) - \frac{1}{2\sigma^2}\sum_{i = 1}^n (y_i-\boldsymbol{x}_i^T\boldsymbol{\beta})^2\right].
\end{split}
$$
Since $x_{i,2}$ (the third row of vector $\boldsymbol{x}_i$) is nonzero for only one term in the sum above, say for  $i = h$, we only need to consider one term. That is, the term with $\boldsymbol{x}_h := [1, x_h, 1]^T$. We also neglect the factor in front since it must be positive. We are left to take the supremum of:

$$
\sup_{\beta_2}-(y_h - \beta_0 - \beta_1x_h - \beta_2)^2,
$$
which means that $\beta_2 = y_h - \beta_0 - \beta_1x_h$. Due to the invariance of MLEs, we now know that
$$
\hat{\beta}_2 = Y_h - \hat{\beta}_0 - \hat{\beta_1}x_h = Y_h - \widehat{Y}_h.
$$
We also note that the estimators $\hat{\beta}_0$ and $\hat{\beta}_1$ are the same here as in the case where we do not consider humans (since the term involving $\boldsymbol{x}_h$ in the log-likelihood evaluates to zero). Thus, since both $T_1, T_2 \propto \hat{\beta}_2$, i.e both $A$ and $B$ occur when the difference $Y_h - \widehat{Y}_h$ is large, we can conclude that they are equivalent. 

\textcolor{red}{More precise than this?}

## d)
For a gamma-distributed random variable, the pdf takes the form
$$
f(x\mid a,b) = \frac{b^a}{\Gamma(a)} x^{a-1}e^{-bx}.
$$
Using the parametrization $\mu = \frac{a}{b}$ and $\nu = a$, we construct the GLM with a log-link as follows. Let the mammalian brain size given body size be given as
$$
y_i \sim \mathrm{Gamma}(\mu_i, \nu),
$$
where 

$$
\ln(\mu_i) = \boldsymbol{x}_i^T \boldsymbol{\beta} =: \eta_i.
$$
Next, we fit the model (note that we use the logarithm of the body mass):
```{r}
mod.gamma <- glm(brain ~ log(body) + is.human, family = Gamma(link = "log"), data = mammals)
summary(mod.gamma)
```

## e)
We want to test whether the following relationship holds:
$$
Y = Y_0M^{3/4},
$$
where $Y$ is the brain mass, $Y_0$ is a constant and $M$ is the brain mass. Since this is equivalent to testing

$$
\ln(Y) = \ln(Y_0) + \frac{3}{4}\ln(M),
$$
we can, for the model in (b), simply perform the hypothesis test:
$$
H_0: \beta_1 = \frac{3}{4} \quad \mathrm{vs.} \quad \beta_1 \ne \frac{3}{4}.
$$
We follow the standard framework for a linear hypothesis test:

```{r}
# Wald test:
C <- matrix(c(0, 1, 0), nrow = 1)
d <-  3/4
r <- 1
p <- 3
n <-  nrow(mammals)
beta1 <- mod1$coefficients[2]
s2 <- deviance(mod1)
X <- model.matrix(mod1)
XtX.inv <- solve(t(X) %*% X)

F.stat <-  (beta1-3/4)^2/(s2*XtX.inv[2,2])
p.val <- pf(F.stat, r, n - p, lower.tail = F)
p.val
```

\textcolor{red}{get wrong p-value??} For a generalized linear model, the Wald statistic can be written as

$$
w =(C\hat{\boldsymbol{\beta}} - d)^T[CF^{-1}(\hat{\boldsymbol{\beta}})C^T]^{-1}(C\hat{\boldsymbol{\beta}} - d),
$$
which is asymptotically $\chi^2$-distributed with $r = \mathrm{rank}(C)$ degrees of freedom. We compute its value:
```{r}
beta <- as.vector(mod.gamma$coefficients)
denom <- solve(C %*% vcov(mod.gamma) %*% t(C))
w <- (C %*% beta - d)^2*denom

p.val <- pchisq(w, r, lower.tail = F)
p.val
```

We perform LRT tests by using an offset term. First we consider the linear model:
```{r}
mod1.offset <- lm(log(brain) ~ 1 + is.human, offset = 3/4*log(body), data = mammals) 
anova(mod1.offset, mod1, test= "Chisq")
```
Then we cosider the GLM
```{r}
mod.gamma.offset <- glm(brain ~ 1 + is.human, family = Gamma(link = "log"), offset = 3/4*log(body), data = mammals) 
anova(mod.gamma.offset, mod.gamma, test= "Chisq")
```
We see that the Wald test and the LRT test differ the most for the linear model. This could be explained by the fact that the Wald test for the lienar model is exact (the test statistic follows an $F$-distribution), while in the LRT test we use an asymptotic distribution. \textcolor{red}{This is wrong. (Right now the p-values are wrong too for some reason..) For one of the models, the LRT and Wald test are equivalent, while not for the other. We need to show this.}

# f)
We need to be careful comparing the log-likelihoods and hence the AICs of the models, because for the GLM we consider $Y \sim \mathrm{Gamma}$, while in the linear model we consider $\ln Y\sim \mathrm{Normal}$. To make them comparable, we define $X:= \ln(Y)$. Then (for the linear model) $Y = e^X$ and the Jacobian transformation yields a density of

$$
f_Y(y) = \left|\frac{\partial x}{\partial y}\right| f_X(x) = \frac{1}{y}f_X(x).
$$
This then yields a log-likelihood:

$$
l_Y(\boldsymbol{\beta}) = l_X(\boldsymbol{\beta}) -\sum_{i = 1}^n \ln y_i,
$$
where  $l_X(\boldsymbol{\beta})$ is the log-likelihood of the original linear model. We implement this 'correction' in the calculation of AIC below: 
```{r}
p = 3
AIC.linear <- 2*p + 2*logLik(mod1) - sum(log(mammals$brain))
AIC.gamma <- 2*p + 2*logLik(mod.gamma)
AIC.linear
AIC.gamma
```
We need to be careful comparing these, because the models have different distributional assumptions. \textcolor{red}{something else maybe? No. It is beceause Y ~ Gamma in one and log(Y) ~ Normal in the other. Need to consider a transformation, e.g. Jacobi transformation?}

The sample skewness can be found as
```{r}
x <- residuals(mod1)
s <- sd(x)
m.3 <- mean(sum(x - mean(x)))
sample.skew <- m.3/s^3
sample.skew
```


\newpage
# Problem 2


## Assumptions

In this problem we apply ordinal multinomial regression to data from Norway Chess 2021. The response variable $y_i$ is the outcome of the $i$'th match. This can be considered an ordered categorical variable
$$
  y_i =
  \begin{cases}
    1 &,\quad \text{white win} \\ 2 &, \quad \text{draw}\\ 3 &, \quad \text{black win},
  \end{cases}
$$
which may depend on relative strength of different players, which player plays white and black and the type of game played. The response can be determined by an underlying latent variable $u_i$, given by
$$
  u_i = -\boldsymbol{x}_i^T \boldsymbol{\beta} + \epsilon_i,
$$
where $\epsilon_i \overset{iid}\sim f$, where $f$ is some standard distribution with cdf $F$. In this model, the event $y_i = r$ occurs if $\theta_{r-1} < u_i \le \theta_{r}$ for some parameters $\{\theta_i\}_{i=0}^3$ satisfying
$$
  -\infty = \theta_0 < \theta_1 < \theta_2 < \theta_3 = \infty.
$$
It follows that
$$
  \text P(y_i \le r) = \text P(u_i \le \theta_r) = \text P(\epsilon_i \le \theta_r + \boldsymbol{x}_i^T \boldsymbol{\beta}) = F(\theta_r + \boldsymbol{x}_i^T \boldsymbol{\beta}),
$$
so the probability of observing a particular outcome of the $i$'th match becomes
$$
  \begin{split}
  \pi_{ir} = P(y_i = r) &= P(y_i \le r) - P(y_i \le r-1)\\
  &= F(\theta_r + \boldsymbol{x}_i^T \boldsymbol{\beta}) - F(\theta_{r-1} + \boldsymbol{x}_i^T \boldsymbol{\beta}).
  \end{split}
$$
This means that our model returns that white wins whenever $u_i \le \theta_1$, draw if $\theta_1 < u_i \le \theta_2$ and black win for $u_i > \theta_2$. 

## Models

### Propositional odds model / Cummulative Logit

$$
  F(x) = \frac{e^x}{1+e^x}, \qquad \epsilon_i \sim \text{Logistic}(0,\ 1)
$$
### Cummulative Probit

$$
  F(x) = \Phi(x), \qquad \epsilon_i \sim N(0,\ 1)
$$
First we consider the model where 
$$
u_i = -(\alpha_{j(i)} + \beta_{l(i)} ) + \varepsilon_i,
$$
where $\alpha_{j(i)}$ is the effect of player $j(i)$ having white pieces, and $\beta_{l(i)}$ is the effect of player $l(i)$ having black pieces.

```{r}
df <- read.csv('data/Norway\ Chess\ 2021.csv')

library(VGAM)
head(df)


fit <- vglm(y ~ factor(white) + factor(black),
            family=cumulative(parallel = TRUE, link="logitlink"), data=df)
AIC(fit)

# P(u <= theta_1), P(u <= theta_2)
p.less_or_equal <- plogis(predict(fit, df))


stats <- cbind('white'=df$white, 'black'=df$black,
               'P(white)'=round(p.less_or_equal[,1],2),
               'P(draw)'=round(p.less_or_equal[,2]-p.less_or_equal[,1],2),
               'P(black)'=round(1-p.less_or_equal[,2],2),
               'outcome'=c('white','draw','black')[df$y])
stats
```

Since it could be argued that a given players skills with one color should be proportional or equal to the skills with another color, we next consider the model where $\alpha_j = \beta_j, \quad j = 1,2,\ldots, k$. The model becomes

$$
u_i = -(\alpha_{j(i)} - \alpha_{l(i)} ) + \varepsilon_i.
$$
\textcolor{red}{Need to drop one column from the design matrix in order to get full rank. Why? Silus says you can imagine that one effect dissapears into the intercept...}
```{r}
library(Matrix)
# The 'simpler' model from the lecture (effect of player being white is equal when being black)
df$black = as.factor(df$black)
df$white = as.factor(df$white)
X = data.frame(matrix(0, nrow(df), nlevels(df$black)))
colnames(X) <- levels(df$black)
for(i in 1:nrow(df)){
  black = as.character(df$black[i])
  white = as.character(df$white[i])
  X[i,black] = 1
  X[i, white] = -1
}
rankMatrix((X))
ncol(X)
# X does not have full rank.

X$type = df$type
X$type = as.factor(X$type)
X$y = df$y

fit.simple <- vglm(y ~ ., family=cumulative(parallel = TRUE, link="logitlink"), data=X[2:ncol(X)])
summary(fit.simple)
AIC(fit.simple)

```

