---
title: 'TMA4315: Project 2'
author: "jototlan@stud.ntnu.no (10018), martigtu@stud.ntnu.no (10037)"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1
```{r}
mammals <- read.table(
  "https://www.math.ntnu.no/~jarlet/statmod/mammals.dat",
  header=T)
```

## a)
```{r}
plot(log(mammals$body), log(mammals$brain)) # Seems pretty linear.
```


A log-log plot of the brain mass against body mass seems to reveal a linear trend. We thus fit the following model:

```{r}
mod0 <- lm(log(brain) ~ log(body), data = mammals)
summary(mod0)
```

## b)

```{r}
is.human = ifelse(mammals$species == "Human", 1, 0)
mammals$is.human = as.factor(is.human)

mod1 <- lm(log(brain) ~ log(body) + is.human, data = mammals)
summary(mod1)
```
Let $\hat{\boldsymbol{\beta}} = [\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2]^T$ be the coefficient estimates given in the summary above. Then the estimated effect on brain mass from being a human is $\hat{\beta}_2 \approx$ `r mod1$coefficients[3]`. Since we have used a log-transform on both the brain mass and body mass, humans will according to the model be larger by a factor of $e^{\hat{\beta}_2} =$ `r exp(mod1$coefficients[3])`.

We use the notation $\boldsymbol{y} = X\boldsymbol{\beta} + \boldsymbol{\varepsilon}$ to represent the linear model. Here, $X$ is the $n \times p$ design matrix, where $n$ is the number of observations and $p$ is the number of parameters used in the model. As usual, $\boldsymbol{\varepsilon} \sim \mathcal{N}(\boldsymbol{0}, \sigma^2 I_n)$. This (along with the other usual assumptions \textcolor{red}{how much detail is required here??}) gives the well known result:

$$
\hat{\boldsymbol{\beta}} \sim \mathcal{N}(\boldsymbol{\beta}, \sigma^2(X^TX)^{-1}).
$$
Now we want to perform the hypothesis test

$$
H_0: \beta_2 = 0 \quad \text{vs.} \quad H_1: \beta_2 > 0.
$$
Under $H_0$, we obtain that (we also index from 0 in the design matrix)

$$
\frac{\hat{\beta}_2}{\sigma\sqrt{(X^TX)^{-1}_{2,2}}} \sim \mathcal{N}(0, 1).
$$

Combining this with the fact that 
$$
\frac{(n-p)s^2}{\sigma^2} \sim \chi^2_{n-p},
$$
where $s^2 = RSS/(n-p)$, we obtain the test statistic

$$
\frac{\hat{\beta}_2}{s\sqrt{(X^TX)^{-1}_{2,2}}} \sim t_{n-p},
$$
under $H_0$. We perform the calculations in R:
```{r}
n <- nrow(mammals)
p <- 3
beta.2 <- mod1$coefficients[3]
s <- sqrt(deviance(mod1)/(n-p))
X <- model.matrix( ~ log(body) + is.human, data = mammals)
XtX.inv <- solve(t(X) %*% X)


T.stat <- beta.2/(s*sqrt(XtX.inv[3,3]))
p.val <- pt(T.stat, n - p, lower.tail = F)
p.val
```
The calculated p-value is `r p.val`.

## c)

We now consider the linear model with only two parameters, $\beta_0$ and $\beta_1$. Let $Y_h = \beta_0 + \beta_1 x_h$ be the stochastic variable from which the log of the human brain mass is realized and $\widehat{Y}_h = \hat{\beta}_0 + \hat{\beta}_1x_h$ be the corresponding estimator. Then we can find the pivotal quantity

$$
T = \frac{Y_h - \widehat{Y}_h}{s\sqrt{1 + 1/n + \frac{(x_h - \bar{x})^2}{\sum_{i = 1}^n(x_i-\bar{x})^2}}} \sim t_{n -2}.
$$
We refer to the good old [\textcolor{blue}{subject-pages}](https://tma4245.math.ntnu.no) (simple linear regression/prediction and prediction intervals in simple linear regression) for this result. Thus, we can find the one-sided prediction interval:

$$
P(T \le k) = 1 - \alpha \implies k = t_{n-2, \alpha}.
$$
Rearranging, we arrive at

$$
P\left(Y_h \le t_{n-2, \alpha} \cdot s\sqrt{1 + 1/n + \frac{(x_h - \bar{x})^2}{\sum_{i = 1}^n(x_i-\bar{x})^2}} + \widehat{Y}_h \right) = 1 - \alpha
$$
We denote the right hand side of the inequality above by $U$ and calculate it with the observed values below
```{r, eval = F}
# HMMM trenegr ikke gjÃ¸re dette da egentlig?
x.h <- mammals$body[mammals$species == "Human"]
mammals.reduced <- mammals[mammals$species != "Human", ]
mod0.reduced <- lm(log(brain) ~ log(body), data = mammals.reduced)
summary(mod0.reduced)
pred.h <- predict(mod0.reduced, newdata = data.fram('body' = x.h)
s <- sqrt(deviance(mod0.reduced))
x.bar <- mean(mammals.reduced$body)
n <- nrow(mammals.reduced)
alpha = 0.05

U <- qt(1 - alpha, n) * s * sqrt(1 + 1/n + (x.h - x.bar)^2/sum((mammals.reduced$body - x.bar)^2) + pred.h
                                
```

\textcolor{red}{very uncertain on this one.}

d)
For a gamma-distributed random variable, the pdf takes the form
$$
f(x\mid a,b) = \frac{b^a}{\Gamma(a)} x^{a-1}e^{-bx}.
$$
Using the parametrization $\mu = \frac{a}{b}$ and $\nu = a$, we construct the GLM with a log-link as follows. Let the mammalian brain size given body size be given as
$$
y_i \sim \mathrm{Gamma}(\mu_i, \nu),
$$
where 

$$
-\frac{1}{\mu_i} = \boldsymbol{x}_i^T \boldsymbol{\beta} =: \eta_i.
$$
Next, we fit the model:
```{r}
mod.glm <- glm(brain ~ log(body) + is.human, family = Gamma(link = "log"), data = mammals)
summary(mod.glm)
```

## e)
We want to test whether the following relationship holds:
$$
Y = Y_0M^{3/4},
$$
where $Y$ is the brain mass, $Y_0$ is a constant and $M$ is the brain mass. Since this is equivalent to testing

$$
\ln(Y) = \ln(Y_0) + \frac{3}{4}\ln(M),
$$
we can, for the model in (b), simply perform the hypothesis test:
$$
H_0: \beta_1 = \frac{3}{4} \quad \mathrm{vs.} \quad \beta_1 \ne \frac{3}{4}.
$$
We follow the standard framework for a linear hypothesis test:

```{r}
# Wald test:
C <- matrix(c(0, 1, 0), nrow = 1)
d <-  3/4
r <- 1
p <- 3
n <-  nrow(mammals)
beta1 <- mod0$coefficients[2]
s2 <- deviance(mod0)
X <- model.matrix( ~ log(body), data = mammals)
XtX.inv <- solve(t(X) %*% X)

F.stat <-  (beta1-3/4)^2/(s2*XtX.inv[2,2])
p.val <- pf(F.stat, r, n - p, lower.tail = F)
p.val
```
\textcolor{red}{How to use LRT test on linear hypothesis? very large p-value!} 
For a generalized linear model, the Wald statistic can be written as

$$
w =(C\hat{\boldsymbol{\beta}} - d)^T[CF^{-1}(\hat{\boldsymbol{\beta}})C^T]^{-1}(C\hat{\boldsymbol{\beta}} - d),
$$
which is asymptotically $\chi^2$-distributed with $r = \mathrm{rank}(C)$ degrees of freedom. We compute its value:
```{r}
beta <- as.vector(mod1$coefficients)
denom <- solve(C %*% vcov(mod1) %*% t(C))
w <- (C %*% beta - d)^2*denom

p.val <- pchisq(w, r, lower.tail = F)
p.val
```

We perform LRT tests by using an offset term. 
```{r}

mod0.o <- lm(log(brain) ~ 1, offset = 3/4*log(body), data = mammals) 
anova(mod0.o, mod0, test= "Chisq")
```


# f)


\newpage
# Problem 2


## Assumptions

In this problem we apply ordinal multinomial regression to data from Norway Chess 2021.

```{r}
df <- read.csv('data/Norway\ Chess\ 2021.csv')
```

The response variable $y_i$ is the outcome of the $i$'th match. This can be considered an ordered categorical variable
$$
  y_i =
  \begin{cases}
    1 &,\quad \text{white win} \\ 2 &, \quad \text{draw}\\ 3 &, \quad \text{black win},
  \end{cases}
$$
which may depend on relative strength of different players, which player plays white and black and the type of game played. The response can be determined by an underlying latent variable $u_i$, given by
$$
  u_i = -\boldsymbol{x}_i^T \boldsymbol{\beta} + \epsilon_i,
$$
where $\epsilon_i \overset{iid}\sim f$, where $f$ is some standard distribution with cdf $F$. In this model, the event $y_i = r$ occurs if $\theta_{r-1} < u_i \le \theta_{r}$ for some parameters $\{\theta_i\}_{i=0}^3$ satisfying
$$
  -\infty = \theta_0 < \theta_1 < \theta_2 < \theta_3 = \infty.
$$
It follows that
$$
  \text P(y_i \le r) = \text P(u_i \le \theta_r) = \text P(\epsilon_i \le \theta_r + \boldsymbol{x}_i^T \boldsymbol{\beta}) = F(\theta_r + \boldsymbol{x}_i^T \boldsymbol{\beta}),
$$
so the probability of observing a particular outcome of the $i$'th match becomes
$$
  \begin{split}
  \pi_{ir} = P(y_i = r) &= P(y_i \le r) - P(y_i \le r-1)\\
  &= F(\theta_r + \boldsymbol{x}_i^T \boldsymbol{\beta}) - F(\theta_{r-1} + \boldsymbol{x}_i^T \boldsymbol{\beta}).
  \end{split}
$$
This means that our model returns that white wins whenever $u_i \le \theta_1$, draw if $\theta_1 < u_i \le \theta_2$ and black win for $u_i > \theta_2$. 

## Models

### Propositional odds model / Cummulative Logit

$$
  F(x) = \frac{e^x}{1+e^x}, \qquad \epsilon_i \sim \text{Logistic}(0,\ 1)
$$
### Cummulative Probit

$$
  F(x) = \Phi(x), \qquad \epsilon_i \sim N(0,\ 1)
$$
### R

```{r}
library(VGAM)
fit <- vglm(y ~ factor(white) + factor(black),
            family=cumulative(parallel = TRUE, link="logitlink"), data=df)

# P(u <= theta_1), P(u <= theta_2)
p.less_or_equal <- plogis(predict(fit, df))


stats <- cbind('white'=df$white, 'black'=df$black,
               'P(white)'=round(p.less_or_equal[,1],2),
               'P(draw)'=round(p.less_or_equal[,2]-p.less_or_equal[,1],2),
               'P(black)'=round(1-p.less_or_equal[,2],2),
               'outcome'=c('white','draw','black')[df$y])
stats
```

